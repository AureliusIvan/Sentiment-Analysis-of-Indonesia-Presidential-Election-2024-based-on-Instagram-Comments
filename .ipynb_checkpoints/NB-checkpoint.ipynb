{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis dari Instagram comments calon Presiden 2024 sebelum dan sesudah deklrasi calon wakil presiden menggunakan Naive Bayes\n",
    "\n",
    "\n",
    "## Tujuan : mengetahui perbedaan sentiment dari komentar instagram sebelum dan sesudah deklarasi calon wakil presiden\n",
    "\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "    Aurelius Ivan Wijaya (00000054769)\n",
    "    </li>\n",
    "    <li>\n",
    "    Rajendra Abhinaya (00000060445)\n",
    "    </li>\n",
    "    <li>\n",
    "    Maecyntha Irelynn Tantra (00000055038)\n",
    "    </li>\n",
    "    <li>\n",
    "    Patricia theodora (00000054093)\n",
    "    </li>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "apa sih yang sebenernya kita cari?\n",
    "* web scrapping algoritm untuk data primer (ivan)\n",
    "* labeling (pat)\n",
    "* data sekunder (mae)\n",
    "* stopword library indonesia (abhi)\n",
    "* cari jurnal referensi yang sudah, sebagai literature review (all, min 4 per person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataest\n",
    "anies_before = pd.read_csv('./Dataset/Anies/anies_before.csv')\n",
    "anies_after = pd.read_csv('./Dataset/Anies/anies_after.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Proccessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Folding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punctuation & Number & Whitespace Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords Removal\n",
    "\n",
    "* Stopwords: Stopwords are common words found in many languages such as prepositions, pronouns, etc that do not add much information\n",
    "\n",
    "Stopword examples in English: I, What, An, The, So\n",
    "\n",
    "Stopword examples in Indonesian: Saya, Dan, Akan, Pada, Jadi\n",
    "\n",
    "* Stopwords removal is the process of removing stopwords from the text in a dataset. This is done to help reduce the amount of words in the dataset which will make training the model faster. As stopwords do not contain any important information, their removal does not negatively impact the model that is being trained.\n",
    "\n",
    "https://towardsdatascience.com/text-pre-processing-stop-words-removal-using-different-libraries-f20bac19929a\n",
    "\n",
    "* Indonesian Stopwords Dataset: https://www.kaggle.com/datasets/oswinrh/indonesian-stoplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ada</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adalah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adanya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adapun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>agak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>agaknya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>wong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>yaitu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>yakin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>yakni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>yang</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>757 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ada\n",
       "0     adalah\n",
       "1     adanya\n",
       "2     adapun\n",
       "3       agak\n",
       "4    agaknya\n",
       "..       ...\n",
       "752     wong\n",
       "753    yaitu\n",
       "754    yakin\n",
       "755    yakni\n",
       "756     yang\n",
       "\n",
       "[757 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = pd.read_csv(\"Dataset/stopwordbahasa.csv\")\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pergi UMN belajar machine learning'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(x):\n",
    "    sentence = []\n",
    "    for word in str(x).split():\n",
    "        #if word is not present in the stopwords dataset then append it to sentence array\n",
    "        if not (stopwords[\"ada\"].eq(word)).any():\n",
    "            sentence.append(word)\n",
    "    #return sentence array that has been joined into a single string\n",
    "    return \" \".join(sentence)\n",
    "\n",
    "#test\n",
    "remove_stopwords(\"saya pergi ke UMN untuk belajar machine learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming / Lemmatization\n",
    "\n",
    "* Stemming : A process that stems or removes common suffixes from a word (“ing,” “ed,” “s,” and “es”).\n",
    "* Lemmatization : Similar to stemming, but sometimes better.\n",
    "\n",
    "For example: \"leafs\" and \"leaves\" :\n",
    "* Stemming : \n",
    "    * leafs -> leaf\n",
    "    * leaves -> leav\n",
    "* Lemmatization :\n",
    "    * leafs -> leaf\n",
    "    * leaves -> leaf\n",
    "\n",
    "---\n",
    "\n",
    "https://medium.com/mlearning-ai/nlp-tokenization-stemming-lemmatization-and-part-of-speech-tagging-9088ac068768\n",
    "\n",
    "Indonesia : https://ksnugroho.medium.com/dasar-text-preprocessing-dengan-python-a4fa52608ffe\n",
    "\n",
    "---\n",
    "\n",
    "keanya bisa dipake sesudah tokenization aja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Sastrawi\n",
      "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
      "Installing collected packages: Sastrawi\n",
      "Successfully installed Sastrawi-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install for the first time\n",
    "# pip install Sastrawi\n",
    "\n",
    "# Sastrawi -> pre processing untuk bahasa indonesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['maaf',\n",
       " 'buat',\n",
       " 'dukung',\n",
       " 'ahy',\n",
       " '',\n",
       " 'beliau',\n",
       " 'blm',\n",
       " 'alam',\n",
       " 'baru',\n",
       " 'citra',\n",
       " '',\n",
       " 'apalagi',\n",
       " 'masuk',\n",
       " 'koalisi',\n",
       " 'hanya',\n",
       " 'ada',\n",
       " 'mau',\n",
       " 'jd',\n",
       " 'bacawapres',\n",
       " 'jd',\n",
       " 'gak',\n",
       " 'tulus',\n",
       " 'ingin',\n",
       " 'rubah',\n",
       " 'negeri',\n",
       " 'ini',\n",
       " 'lbh',\n",
       " 'baik',\n",
       " '',\n",
       " 'pikir',\n",
       " 'jernih',\n",
       " 'siapa',\n",
       " 'yg',\n",
       " 'benar',\n",
       " 'yg',\n",
       " 'penghianat',\n",
       " '',\n",
       " 'jangan',\n",
       " 'mudah',\n",
       " 'hanya',\n",
       " 'krn',\n",
       " 'bapak',\n",
       " 'rasa',\n",
       " 'tdk',\n",
       " 'ajak',\n",
       " 'bicara',\n",
       " '',\n",
       " 'prinsip',\n",
       " 'utama',\n",
       " 'itu',\n",
       " 'dukung',\n",
       " 'pak',\n",
       " 'anies',\n",
       " 'dg',\n",
       " 'ubah',\n",
       " 'tanpa',\n",
       " 'hrs',\n",
       " 'lihat',\n",
       " 'siapa',\n",
       " 'damping',\n",
       " 'dan',\n",
       " 'menang',\n",
       " 'dapat',\n",
       " 'suara',\n",
       " 'banyak',\n",
       " 'dari',\n",
       " 'kaum',\n",
       " 'nahdliyin',\n",
       " '']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# Get the array ready to contan stemmed words\n",
    "stemming_words = []\n",
    "\n",
    "for word in tokens_words:\n",
    "    stemming_words.append(stemmer.stem(word))\n",
    "    \n",
    "stemming_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Normalization / Noise Removal\n",
    "\n",
    "* this one need research (ivan)\n",
    "* slang word dataset that i used : https://github.com/nasalsabila/kamus-alay\n",
    "\n",
    "* Contoh sebelum: \"Para mahasiswa yang memperoleh nilai yang rendah dalam ujian tidak diizinkan untuk mengikuti ujian ulang.\"\n",
    "* Contoh sesudah: \"Mahasiswa yang memperoleh nilai rendah dalam ujian tidak diizinkan mengikuti ujian ulang.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "indo_slang_word = pd.read_csv('./Dataset/TextNormalization/colloquial-indonesian-lexicon.csv')\n",
    "indo_slang_word.head()\n",
    "\n",
    "def replace_slang_word(doc,slang_word):\n",
    "    for index in  range(0,len(doc)-1):\n",
    "        index_slang = slang_word.slang==doc[index]\n",
    "        formal = list(set(slang_word[index_slang].formal))\n",
    "        if len(formal)==1:\n",
    "            doc[index]=formal[0]\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "* Simply, a process of breaking / seperating sentences into words.\n",
    "* Tokenization can be done by using either of these libraries, textblob or ntlk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Maaf', 'buat', 'pendukung', 'AHY', ',', 'beliau', 'blm', 'pengalaman', 'baru', 'pencitraan', '....', 'apalagi', 'masuk', 'koalisi', 'hanya', 'ada', 'maunya', 'jd', 'bacawapres', 'jd', 'gak', 'tulus', 'ingin', 'merubah', 'negeri', 'ini', 'lbh', 'baik', '.', 'Berpikirlah', 'jernih', 'siapa', 'yg', 'sebenarnya', 'yg', 'penghianat', ',', 'jangan', 'mudah', 'hanya', 'krn', 'bapaknya', 'merasa', 'tdk', 'diajak', 'bicara', '.', 'Prinsip', 'utamanya', 'itu', 'mendukung', 'pak', 'Anies', 'dg', 'perubahannya', 'tanpa', 'hrs', 'melihat', 'siapa', 'pendampingnya', 'dan', 'menang', 'mendapatkan', 'suara', 'terbanyak', 'dari', 'kaum', 'nahdliyin', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the 'punkt' data\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk import sent_tokenize # split the text into sentence\n",
    "from nltk import word_tokenize\n",
    "\n",
    "text = anies_before.iloc[4, 6]\n",
    "\n",
    "# Won't be needed probably (?)\n",
    "# Seperate text into sentences\n",
    "# tokens_sents = nltk.sent_tokenize(text)\n",
    "# print(tokens_sents)\n",
    "\n",
    "# print()\n",
    "\n",
    "# Seperate sentence into words\n",
    "tokens_words = nltk.word_tokenize(text)\n",
    "print(tokens_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Minority Oversampling Technique (SMOTE)\n",
    "\n",
    "* this one need research (ivan)\n",
    "* Smote adalah sebuah tehnik yang digunakan terhadap data yang tidak seimbang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE()\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_tweets_tfidf, y_train.values)\n",
    "print(X_train_smote.shape, y_train_smote.shape)\n",
    "\n",
    "# SMOTE on full training data\n",
    "smote = SMOTE()\n",
    "X_smote, y_smote = smote.fit_resample(X_tweets_tfidf, y.values)\n",
    "print(X_smote.shape, y_smote.shape)\n",
    "\n",
    "# Class Imbalance Check\n",
    "plt.pie(pd.value_counts(y_train_smote), \n",
    "        labels=['Label 0 (Positive)', 'Label 1 (Negative)'], \n",
    "        autopct='%0.1f%%')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation\n",
    "\n",
    "* this one need research (abhi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workload Distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "<!-- * kemungkinan Binomial Naive Bayes -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "* this one need research (pat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
