{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis dari Instagram comments calon Presiden 2024 sebelum dan sesudah deklrasi calon wakil presiden menggunakan Naive Bayes\n",
    "\n",
    "\n",
    "## Tujuan : mengetahui perbedaan sentiment dari komentar instagram sebelum dan sesudah deklarasi calon wakil presiden\n",
    "\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "    Aurelius Ivan Wijaya (00000054769)\n",
    "    </li>\n",
    "    <li>\n",
    "    Rajendra Abhinaya (00000060445)\n",
    "    </li>\n",
    "    <li>\n",
    "    Maecyntha Irelynn Tantra (00000055038)\n",
    "    </li>\n",
    "    <li>\n",
    "    Patricia theodora (00000054093)\n",
    "    </li>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "apa sih yang sebenernya kita cari?\n",
    "* web scrapping algoritm untuk data primer (ivan)\n",
    "* labeling (pat)\n",
    "* data sekunder (mae)\n",
    "* stopword library indonesia (abhi)\n",
    "* cari jurnal referensi yang sudah, sebagai literature review (all, min 4 per person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset    \n",
    "# [ Primary Dataset ]\n",
    "# anies_before = pd.read_csv('./Dataset/Anies/anies_before.csv', on_bad_lines='skip')\n",
    "# anies_after = pd.DataFrame(pd.read_csv('./Dataset/Anies/anies_after.csv', on_bad_lines=\"skip\"))\n",
    "ganjar_before = pd.DataFrame(pd.read_csv('./Dataset/Ganjar/ganjar_before.csv', on_bad_lines=\"skip\")) # get only 100 data\n",
    "ganjar_before = ganjar_before[:100]\n",
    "ganjar_after = pd.DataFrame(pd.read_csv('./Dataset/Ganjar/ganjar_after.csv', on_bad_lines=\"skip\"))\n",
    "ganjar_after = ganjar_after[:100]\n",
    "prabowo_before = pd.DataFrame(pd.read_csv('./Dataset/Prabowo/prabowo_before.csv', on_bad_lines=\"skip\"))\n",
    "prabowo_before = prabowo_before[:100]\n",
    "prabowo_after = pd.DataFrame(pd.read_csv('./Dataset/Prabowo/prabowo_after.csv', on_bad_lines=\"skip\"))\n",
    "prabowo_after = prabowo_after[:100]\n",
    "# prabowo_before = pd.DataFrame(pd.read_csv('./Dataset/Detik/detik_prabowo_before.csv', on_bad_lines=\"skip\"))\n",
    "# prabowo_after = pd.DataFrame(pd.read_csv('./Dataset/Detik/detik_prabowo_after.csv', on_bad_lines=\"skip\"   ))\n",
    "detik_anies_before = pd.read_csv('./Dataset/Detik/detik_anies_before.csv', on_bad_lines='skip')\n",
    "detik_anies_after = pd.DataFrame(pd.read_csv('./Dataset/Detik/detik_anies_before.csv', on_bad_lines=\"skip\"))\n",
    "detik_ganjar_before = pd.DataFrame(pd.read_csv('./Dataset/Detik/detik_ganjar_before.csv', on_bad_lines=\"skip\"))\n",
    "detik_ganjar_after = pd.DataFrame(pd.read_csv('./Dataset/Detik/detik_ganjar_after.csv', on_bad_lines=\"skip\"))\n",
    "detik_prabowo_before = pd.DataFrame(pd.read_csv('./Dataset/Detik/detik_prabowo_before.csv', on_bad_lines=\"skip\"))\n",
    "detik_prabowo_after = pd.DataFrame(pd.read_csv('./Dataset/Detik/detik_prabowo_after.csv', on_bad_lines=\"skip\"   ))\n",
    "# [ Secondary Dataset ]\n",
    "instagram_cyber_comments = pd.read_csv('./Dataset/dataset_komentar_instagram_cyberbullying.csv')\n",
    "tweet_tv = pd.read_csv('./Dataset/dataset_tweet_sentimen_tayangan_tv.csv')\n",
    "tweet_pilkada = pd.read_csv('./Dataset/dataset_tweet_sentiment_pilkada_DKI_2017.csv')\n",
    "tweet_opini_film = pd.read_csv('./Dataset/dataset_tweet_sentiment_opini_film.csv')\n",
    "tweet_cellular = pd.read_csv('./Dataset/dataset_tweet_sentiment_cellular_service_provider.csv')\n",
    "prastyo_sentiment = pd.read_csv('./Dataset/prastyo-sentiment_all.csv')\n",
    "sentiment_twitter_indonesia = pd.read_csv('./Dataset/Indonesian Sentiment Twitter Dataset Labeled.csv', sep='\\t')\n",
    "# sentiment_shopee = pd.read_csv('./Dataset/dataset_shopee2.csv', on_bad_lines=\"skip\")\n",
    "import chardet\n",
    "\n",
    "# Detect encoding\n",
    "with open('./Dataset/dataset_shopee2.csv', 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "# Use the detected encoding when reading the CSV file\n",
    "sentiment_shopee = pd.read_csv('./Dataset/dataset_shopee2.csv', on_bad_lines=\"skip\", encoding=result['encoding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def balance_dataset(data, label_column, max_samples=None):\n",
    "\n",
    "    # Determine the minimum number of samples for any label\n",
    "    min_samples = data[label_column].value_counts().min()\n",
    "\n",
    "    # Randomly select min_samples samples for each label\n",
    "    balanced_subset = pd.concat([group.sample(min_samples) for _, group in data.groupby(label_column)])\n",
    "\n",
    "    # Shuffle the balanced subset to mix the labels\n",
    "    balanced_subset = balanced_subset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Optionally only keep a maximum number of samples\n",
    "    if max_samples is not None:\n",
    "        balanced_subset = balanced_subset[:max_samples]\n",
    "    \n",
    "    return balanced_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: positive, 0: negative \n",
    "\n",
    "# data integration\n",
    "# change label name to 'comments'\n",
    "instagram_cyber_comments.rename(columns={'Instagram Comment Text': 'comments'}, inplace=True)\n",
    "# mapping sentiment\n",
    "instagram_cyber_comments['Sentiment'] = instagram_cyber_comments['Sentiment'].map({'positive': 1, 'negative': 0})\n",
    "instagram_cyber_comments['label'] = instagram_cyber_comments['Sentiment'].astype(int)\n",
    "# drop unused columns\n",
    "instagram_cyber_comments.drop(columns=['Id', 'Sentiment'], inplace=True)\n",
    "# display(instagram_cyber_comments.head())\n",
    "#change data type to string to ensure all data type is string\n",
    "instagram_cyber_comments['comments'] = instagram_cyber_comments['comments'].astype(str)\n",
    "\n",
    "# tweet_tv\n",
    "# change label name to 'comments'\n",
    "tweet_tv.rename(columns={'Tweet': 'comments'}, inplace=True)\n",
    "# mapping sentiment\n",
    "tweet_tv['Sentiment'] = tweet_tv['Sentiment'].map({'positive': 1, 'negative': 0})\n",
    "tweet_tv['label'] = tweet_tv['Sentiment'].astype(int)\n",
    "tweet_tv.rename(columns={'Text Tweet': 'comments'}, inplace=True)\n",
    "tweet_tv.drop(columns=['Id', 'Sentiment', \"Jumlah Retweet\", \"Acara TV\"], inplace=True)\n",
    "#change data type to string to ensure all data type is string\n",
    "tweet_tv['comments'] = tweet_tv['comments'].astype(str)\n",
    "\n",
    "# tweet_pilkada\n",
    "# tweet_pilkada add new column 'label'\n",
    "# tweet_pilkada['label'] = tweet_pilkada['Sentiment'].map({'positive': 1, 'negative': 0}).astype(int)\n",
    "tweet_pilkada['Sentiment'] = tweet_pilkada['Sentiment'].map({'positive': 1, 'negative': 0}).astype(int)\n",
    "# tweet_pilkada['label'] = tweet_pilkada['Sentiment'].astype(int)\n",
    "tweet_pilkada = tweet_pilkada[['Sentiment', 'Text Tweet']]\n",
    "tweet_pilkada.rename(columns={'Text Tweet': 'comments'}, inplace=True)\n",
    "tweet_pilkada.rename(columns={'Sentiment': 'label'}, inplace=True)\n",
    "#change data type to string to ensure all data type is string\n",
    "tweet_pilkada['comments'] = tweet_pilkada['comments'].astype(str)\n",
    "\n",
    "# tweet_opini_film\n",
    "# change label name to 'comments'\n",
    "tweet_opini_film.rename(columns={'Tweet': 'comments'}, inplace=True)\n",
    "# mapping sentiment\n",
    "tweet_opini_film['Sentiment'] = tweet_opini_film['Sentiment'].map({'positive': 1, 'negative': 0})\n",
    "tweet_opini_film['label'] = tweet_opini_film['Sentiment'].astype(int)\n",
    "tweet_opini_film.drop(columns=['Id', 'Sentiment'], inplace=True)\n",
    "tweet_opini_film.rename(columns={'Text Tweet': 'comments'}, inplace=True)\n",
    "#change data type to string to ensure all data type is string\n",
    "tweet_opini_film['comments'] = tweet_opini_film['comments'].astype(str)\n",
    "\n",
    "# tweet_cellular\n",
    "# change label name to 'comments'\n",
    "tweet_cellular.rename(columns={'Tweet': 'comments'}, inplace=True)\n",
    "# mapping sentiment\n",
    "tweet_cellular['Sentiment'] = tweet_cellular['Sentiment'].map({'positive': 1, 'negative': 0})\n",
    "tweet_cellular['label'] = tweet_cellular['Sentiment'].astype(int)\n",
    "tweet_cellular.drop(columns=['Id', 'Sentiment'], inplace=True)\n",
    "tweet_cellular.rename(columns={'Text Tweet': 'comments'}, inplace=True)\n",
    "\n",
    "# prastyo_sentiment\n",
    "map = {'pos': 1, 'neg': 0, 'neu': 1}\n",
    "prastyo_sentiment['label'] = prastyo_sentiment['label'].map(map)\n",
    "\n",
    "\n",
    "# sentiment twitter indonesia\n",
    "# change label name to 'comments'\n",
    "sentiment_twitter_indonesia.rename(columns={'Tweet': 'comments'}, inplace=True)\n",
    "# mapping sentiment\n",
    "# sentiment_twitter_indonesia['sentimen'] = sentiment_twitter_indonesia['sentimen'].map({'positive': 1, 'negative': 0})\n",
    "sentiment_twitter_indonesia['label'] = sentiment_twitter_indonesia['sentimen'].astype(int)\n",
    "\n",
    "\n",
    "# shoppe\n",
    "# change label name to 'comments'\n",
    "sentiment_shopee.rename(columns={'Review': 'comments'}, inplace=True)\n",
    "# mapping sentiment\n",
    "sentiment_shopee['SENTIMEN'] = sentiment_shopee['SENTIMEN'].map({'POSITIF': 1, 'NEGATIF': 0})\n",
    "sentiment_shopee['label'] = sentiment_shopee['SENTIMEN'].astype(int)\n",
    "sentiment_shopee.drop(columns=['SENTIMEN'], inplace=True)\n",
    "\n",
    "\n",
    "# detik_anies_after = balance_dataset(detik_anies_after, 'label')\n",
    "# detik_anies_before = balance_dataset(detik_anies_before, 'label')\n",
    "# detik_ganjar_before = balance_dataset(detik_ganjar_before, 'label', max_samples=50)\n",
    "detik_ganjar_after = balance_dataset(detik_ganjar_after, 'label', max_samples=50)\n",
    "tweet_pilkada = balance_dataset(tweet_pilkada, 'label')\n",
    "\n",
    "Training_dataset = pd.read_csv('./Dataset/Training/Training.csv', on_bad_lines='skip')\n",
    "# only pick comments and label column\n",
    "Training_dataset = Training_dataset[['comments', 'label']]\n",
    "# check training \n",
    "# balance\n",
    "Training_dataset = balance_dataset(Training_dataset, 'label')\n",
    "\n",
    "secondary_dataset = pd.concat([\n",
    "   Training_dataset,\n",
    "    ], ignore_index=True)\n",
    "# only pick comments and label column\n",
    "secondary_dataset = secondary_dataset[['comments', 'label']]\n",
    "#change data type to string to ensure all data type is string\n",
    "secondary_dataset['comments'] = secondary_dataset['comments'].astype(str)\n",
    "# if there is column 'Unnamed: 0' in secondary dataset, drop it\n",
    "if 'Unnamed: 0' in secondary_dataset.columns:\n",
    "    secondary_dataset.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Proccessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 64 entries, 0 to 63\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   comments  64 non-null     object \n",
      " 1   label     64 non-null     float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 1.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anis baswedan presiden indonesia membuat perub...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ganjar -aldi Taher menang 1 putaran</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Siapapun calon WAPRESNYA sy dan keluarga sudah...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kalo jadi sama pak @sandiuno bisa langsung men...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kalo mau indonesia hancur, pilih dia sebagai p...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            comments  label\n",
       "0  anis baswedan presiden indonesia membuat perub...    1.0\n",
       "1                Ganjar -aldi Taher menang 1 putaran    1.0\n",
       "2  Siapapun calon WAPRESNYA sy dan keluarga sudah...    1.0\n",
       "3  Kalo jadi sama pak @sandiuno bisa langsung men...    1.0\n",
       "4  Kalo mau indonesia hancur, pilih dia sebagai p...    0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(secondary_dataset.info())\n",
    "display(secondary_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 64 entries, 0 to 63\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   comments  64 non-null     object \n",
      " 1   label     64 non-null     float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 1.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anis baswedan presiden indonesia membuat perub...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ganjar -aldi Taher menang 1 putaran</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Siapapun calon WAPRESNYA sy dan keluarga sudah...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kalo jadi sama pak @sandiuno bisa langsung men...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kalo mau indonesia hancur, pilih dia sebagai p...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            comments  label\n",
       "0  anis baswedan presiden indonesia membuat perub...    1.0\n",
       "1                Ganjar -aldi Taher menang 1 putaran    1.0\n",
       "2  Siapapun calon WAPRESNYA sy dan keluarga sudah...    1.0\n",
       "3  Kalo jadi sama pak @sandiuno bisa langsung men...    1.0\n",
       "4  Kalo mau indonesia hancur, pilih dia sebagai p...    0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def handleMissingValue(df):\n",
    "    df = df.dropna()\n",
    "    df = df.drop_duplicates()\n",
    "    df = df[df['comments'] != '']\n",
    "    # remove 'NaN' value\n",
    "    if('NaN' in df['comments']):\n",
    "        df = df[df['comments'] != 'NaN']\n",
    "    df = df[df['comments'].notna()]\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df[['comments']] \n",
    "    return df\n",
    "secondary_dataset['comments'] = handleMissingValue(secondary_dataset)\n",
    "display(secondary_dataset.info())\n",
    "display(secondary_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comments    0\n",
      "label       0\n",
      "dtype: int64\n",
      "comments    0\n",
      "label       0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anis baswedan presiden indonesia membuat perub...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ganjar -aldi Taher menang 1 putaran</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Siapapun calon WAPRESNYA sy dan keluarga sudah...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kalo jadi sama pak @sandiuno bisa langsung men...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kalo mau indonesia hancur, pilih dia sebagai p...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Semoga Allah SWT Menolong kita ü§≤ü§≤</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>petugas Partai bukan calon presiden,,,</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Pak, saya cinta banget sama bapak, dari awal s...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Coba sama pak bambang pasti cocok..</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Pak bagusnya Pasangannya ahoküëè</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comments  label\n",
       "0   anis baswedan presiden indonesia membuat perub...    1.0\n",
       "1                 Ganjar -aldi Taher menang 1 putaran    1.0\n",
       "2   Siapapun calon WAPRESNYA sy dan keluarga sudah...    1.0\n",
       "3   Kalo jadi sama pak @sandiuno bisa langsung men...    1.0\n",
       "4   Kalo mau indonesia hancur, pilih dia sebagai p...    0.0\n",
       "..                                                ...    ...\n",
       "59                  Semoga Allah SWT Menolong kita ü§≤ü§≤    1.0\n",
       "60             petugas Partai bukan calon presiden,,,    0.0\n",
       "61  Pak, saya cinta banget sama bapak, dari awal s...    1.0\n",
       "62                Coba sama pak bambang pasti cocok..    1.0\n",
       "63                     Pak bagusnya Pasangannya ahoküëè    1.0\n",
       "\n",
       "[64 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(secondary_dataset.isnull().sum())\n",
    "# delete duplicate data and missing value and null value\n",
    "secondary_dataset = secondary_dataset.dropna()\n",
    "secondary_dataset = secondary_dataset.drop_duplicates()\n",
    "secondary_dataset = secondary_dataset[secondary_dataset['comments'] != '']\n",
    "# remove 'NaN' value\n",
    "if('NaN' in secondary_dataset['comments']):\n",
    "    secondary_dataset = secondary_dataset[secondary_dataset['comments'] != 'NaN']\n",
    "secondary_dataset = secondary_dataset[secondary_dataset['comments'].notna()]\n",
    "secondary_dataset = secondary_dataset.reset_index(drop=True)\n",
    "# secondary_dataset = secondary_dataset[['comments']]\n",
    "print(secondary_dataset.isnull().sum())\n",
    "\n",
    "display(secondary_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Precompile regex patterns\n",
    "re_long_word = re.compile(r'\\b\\w{20,}\\b')\n",
    "re_non_ascii = re.compile(r'[^\\x00-\\x7F]+')\n",
    "# re_punctuation = re.compile(f\"[{re.escape(string.punctuation)}]\")\n",
    "re_single_char = re.compile(r'\\s+[a-zA-Z]\\s+')\n",
    "re_numbers = re.compile(r'\\d+')\n",
    "re_tags = re.compile(\"&lt;/?.*?&gt;\")\n",
    "re_special_chars_digits = re.compile(\"(\\\\d|\\\\W)+\")\n",
    "re_repeated_chars = re.compile(r'(.)\\1+')\n",
    "\n",
    "import re\n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)\n",
    "\n",
    "def clean_text(comment):\n",
    "    comment = str(comment)\n",
    "    comment = comment.lower() # Case Folding to lowercase\n",
    "\n",
    "    # Remove non-ascii words and characters\n",
    "    comment = re_non_ascii.sub(' ', comment) # remove non-ascii characters\n",
    "\n",
    "    # Remove all links\n",
    "    comment = re.sub(r\"http\\S+\", \"\", comment) # remove all links\n",
    "\n",
    "    # Delete all emoticons from text\n",
    "    comment = re.sub(':[^ ]+:', '', comment) # remove emoticons\n",
    "    comment = remove_emojis(comment)\n",
    "    # remove all word that start with < and end with >\n",
    "    comment = re.sub('<[^>]*>', '', comment)\n",
    "    # Remove long words\n",
    "    comment = re_long_word.sub('', comment) # remove long words\n",
    "\n",
    "    # if length of comment is more than 7 and ended with \"ny\" convert it to \"nya\"\n",
    "    if(len(comment) > 7 and comment[-2:] == 'ny'):\n",
    "        # comment = comment[:-2] + 'nya'\n",
    "        # add nya to the end of word\n",
    "        comment = re.sub(r'\\b(\\w+)', r'\\1nya', comment)\n",
    "    # Remove single characters\n",
    "    comment = re_single_char.sub(' ', comment) # remove single characters\n",
    "\n",
    "    # Remove numbers\n",
    "    comment = re_numbers.sub('', comment)\n",
    "\n",
    "    # Remove word that contains tags\n",
    "    comment = re_tags.sub('', comment)\n",
    "\n",
    "    # Remove special characters and digits\n",
    "    comment = re_special_chars_digits.sub(\" \", comment) \n",
    "\n",
    "    # If a letter is repeated more than 2 times, replace it with 1 time\n",
    "    comment = re_repeated_chars.sub(r'\\1\\1', comment)\n",
    "    # if comment is empty or only contain one word, return empty string\n",
    "    if(len(comment) <= 1):\n",
    "        return ''\n",
    "    # Remove extra whitespaces\n",
    "    comment = re.sub(r'\\s+', ' ', comment)\n",
    "\n",
    "    return comment.strip()\n",
    "\n",
    "def remove_punct(data):\n",
    "    data['comments'] = data['comments'].apply(clean_text)\n",
    "    # remove nan value\n",
    "    data = data[data['comments'] != '']\n",
    "    data = data[data['comments'].notna()]\n",
    "    data = data.reset_index(drop=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comments    0\n",
      "label       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Call the function\n",
    "secondary_dataset.to_csv('./dirty_data.csv', index=False)\n",
    "cleaned_data = remove_punct(secondary_dataset)\n",
    "secondary_dataset['comments'] = cleaned_data['comments']\n",
    "\n",
    "secondary_dataset = secondary_dataset[secondary_dataset['comments'] != '']\n",
    "# remove 'NaN' value\n",
    "if('NaN' in secondary_dataset['comments']):\n",
    "    secondary_dataset = secondary_dataset[secondary_dataset['comments'] != 'NaN']\n",
    "secondary_dataset = secondary_dataset[secondary_dataset['comments'].notna()]\n",
    "secondary_dataset = secondary_dataset.reset_index(drop=True)\n",
    "\n",
    "# secondary_dataset = secondary_dataset[['comments']]\n",
    "print(secondary_dataset.isnull().sum())\n",
    "# Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anis baswedan presiden indonesia membuat perub...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ganjar aldi taher menang putaran</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>siapapun calon wapresnya sy dan keluarga sudah...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kalo jadi sama pak sandiuno bisa langsung mena...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kalo mau indonesia hancur pilih dia sebagai pr...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>semoga allah swt menolong kita</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>petugas partai bukan calon presiden</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>pak saya cinta banget sama bapak dari awal say...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>coba sama pak bambang pasti cocok</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>pak bagusnya pasangannya ahok</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comments  label\n",
       "0   anis baswedan presiden indonesia membuat perub...    1.0\n",
       "1                    ganjar aldi taher menang putaran    1.0\n",
       "2   siapapun calon wapresnya sy dan keluarga sudah...    1.0\n",
       "3   kalo jadi sama pak sandiuno bisa langsung mena...    1.0\n",
       "4   kalo mau indonesia hancur pilih dia sebagai pr...    0.0\n",
       "..                                                ...    ...\n",
       "59                     semoga allah swt menolong kita    1.0\n",
       "60                petugas partai bukan calon presiden    0.0\n",
       "61  pak saya cinta banget sama bapak dari awal say...    1.0\n",
       "62                  coba sama pak bambang pasti cocok    1.0\n",
       "63                      pak bagusnya pasangannya ahok    1.0\n",
       "\n",
       "[64 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(secondary_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anis baswedan presiden indonesia membuat perub...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ganjar aldi taher menang putaran</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>siapapun calon wapresnya sy dan keluarga sudah...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kalo jadi sama pak sandiuno bisa langsung mena...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kalo mau indonesia hancur pilih dia sebagai pr...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>semoga allah swt menolong kita</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>petugas partai bukan calon presiden</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>pak saya cinta banget sama bapak dari awal say...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>coba sama pak bambang pasti cocok</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>pak bagusnya pasangannya ahok</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comments  label\n",
       "0   anis baswedan presiden indonesia membuat perub...    1.0\n",
       "1                    ganjar aldi taher menang putaran    1.0\n",
       "2   siapapun calon wapresnya sy dan keluarga sudah...    1.0\n",
       "3   kalo jadi sama pak sandiuno bisa langsung mena...    1.0\n",
       "4   kalo mau indonesia hancur pilih dia sebagai pr...    0.0\n",
       "..                                                ...    ...\n",
       "59                     semoga allah swt menolong kita    1.0\n",
       "60                petugas partai bukan calon presiden    0.0\n",
       "61  pak saya cinta banget sama bapak dari awal say...    1.0\n",
       "62                  coba sama pak bambang pasti cocok    1.0\n",
       "63                      pak bagusnya pasangannya ahok    1.0\n",
       "\n",
       "[64 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "display(secondary_dataset)\n",
    "# Stopword removal\n",
    "\n",
    "stopword_bahasa = pd.read_csv('./Dataset/TextNormalization/stopwordbahasa.csv', header=None)\n",
    "\n",
    "# def stopword_removal(data):\n",
    "#     for i in range(len(data)):\n",
    "#         data = data.replace(stopword_bahasa[0][i], '')\n",
    "#     return data\n",
    "\n",
    "def stopword_removal(data):\n",
    "    if isinstance(data, str):\n",
    "        # Split the string into a list of words\n",
    "        words = data.split()\n",
    "        # Remove stop words\n",
    "        words = [word for word in words if word not in stopword_bahasa[0].values]\n",
    "        # Join the list of words back into a string\n",
    "        return ' '.join(words)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>presiden indonesia perubahan bangsa indonesia ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aldi menang putaran</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>calon wapresnya keluarga pilih</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kalo langsung menang tampa berputar putar</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kalo indonesia hancur pilih presiden</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>semoga allah swt menolong</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>petugas partai calon presiden</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>cinta banget umur pemilihan dukung smpe sehat ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>coba bambang cocok</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>bagusnya pasangannya</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comments  label\n",
       "0   presiden indonesia perubahan bangsa indonesia ...    1.0\n",
       "1                                 aldi menang putaran    1.0\n",
       "2                      calon wapresnya keluarga pilih    1.0\n",
       "3           kalo langsung menang tampa berputar putar    1.0\n",
       "4                kalo indonesia hancur pilih presiden    0.0\n",
       "..                                                ...    ...\n",
       "59                          semoga allah swt menolong    1.0\n",
       "60                      petugas partai calon presiden    0.0\n",
       "61  cinta banget umur pemilihan dukung smpe sehat ...    1.0\n",
       "62                                 coba bambang cocok    1.0\n",
       "63                               bagusnya pasangannya    1.0\n",
       "\n",
       "[64 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>presiden indonesia perubahan bangsa indonesia ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aldi menang putaran</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>calon wapresnya keluarga pilih</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kalo langsung menang tampa berputar putar</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kalo indonesia hancur pilih presiden</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>semoga allah swt menolong</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>petugas partai calon presiden</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>cinta banget umur pemilihan dukung smpe sehat ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>coba bambang cocok</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>bagusnya pasangannya</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comments  label\n",
       "0   presiden indonesia perubahan bangsa indonesia ...    1.0\n",
       "1                                 aldi menang putaran    1.0\n",
       "2                      calon wapresnya keluarga pilih    1.0\n",
       "3           kalo langsung menang tampa berputar putar    1.0\n",
       "4                kalo indonesia hancur pilih presiden    0.0\n",
       "..                                                ...    ...\n",
       "59                          semoga allah swt menolong    1.0\n",
       "60                      petugas partai calon presiden    0.0\n",
       "61  cinta banget umur pemilihan dukung smpe sehat ...    1.0\n",
       "62                                 coba bambang cocok    1.0\n",
       "63                               bagusnya pasangannya    1.0\n",
       "\n",
       "[64 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Call the function\n",
    "# cleaned_data = stopword_removal(secondary_dataset)\n",
    "# convert comments to string\n",
    "# secondary_dataset['comments'] = secondary_dataset['comments'].astype(str)\n",
    "\n",
    "#check null value\n",
    "# secondary_dataset['comments'] = secondary_dataset['comments'].apply(stopword_removal)\n",
    "secondary_dataset['comments'] = secondary_dataset['comments'].apply(lambda x: stopword_removal(x))\n",
    "# secondary_dataset['comments'] = secondary_dataset['comments'].apply(lambda x: stopword_removal(x))\n",
    "display(secondary_dataset)\n",
    "\n",
    "# handle null, missing value, and duplicate data\n",
    "secondary_dataset = secondary_dataset.dropna()\n",
    "secondary_dataset = secondary_dataset.drop_duplicates()\n",
    "secondary_dataset = secondary_dataset[secondary_dataset['comments'] != '']\n",
    "# remove 'NaN' value\n",
    "if('NaN' in secondary_dataset['comments']):\n",
    "    secondary_dataset = secondary_dataset[secondary_dataset['comments'] != 'NaN']\n",
    "secondary_dataset = secondary_dataset[secondary_dataset['comments'].notna()]\n",
    "secondary_dataset = secondary_dataset.reset_index(drop=True)\n",
    "# secondary_dataset = secondary_dataset[['comments']]\n",
    "display(secondary_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aurel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[presiden, indonesia, perubahan, bangsa, indon...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[aldi, menang, putaran]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[calon, wapresnya, keluarga, pilih]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[kalo, langsung, menang, tampa, berputar, putar]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[kalo, indonesia, hancur, pilih, presiden]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>[semoga, allah, swt, menolong]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>[petugas, partai, calon, presiden]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>[cinta, banget, umur, pemilihan, dukung, smpe,...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>[coba, bambang, cocok]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>[bagusnya, pasangannya]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comments  label\n",
       "0   [presiden, indonesia, perubahan, bangsa, indon...    1.0\n",
       "1                             [aldi, menang, putaran]    1.0\n",
       "2                 [calon, wapresnya, keluarga, pilih]    1.0\n",
       "3    [kalo, langsung, menang, tampa, berputar, putar]    1.0\n",
       "4          [kalo, indonesia, hancur, pilih, presiden]    0.0\n",
       "..                                                ...    ...\n",
       "59                     [semoga, allah, swt, menolong]    1.0\n",
       "60                 [petugas, partai, calon, presiden]    0.0\n",
       "61  [cinta, banget, umur, pemilihan, dukung, smpe,...    1.0\n",
       "62                             [coba, bambang, cocok]    1.0\n",
       "63                            [bagusnya, pasangannya]    1.0\n",
       "\n",
       "[64 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenization\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "secondary_dataset['comments'] = secondary_dataset['comments'].apply(word_tokenize)\n",
    "\n",
    "display(secondary_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5045, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slang</th>\n",
       "      <th>formal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22nya</td>\n",
       "      <td>dua-duanya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaaaaaakkk</td>\n",
       "      <td>ah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaaaahhhh</td>\n",
       "      <td>ah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aaaammmiiiinnnn</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aaahh</td>\n",
       "      <td>ah</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             slang      formal\n",
       "0            22nya  dua-duanya\n",
       "1       aaaaaaakkk          ah\n",
       "2        aaaaahhhh          ah\n",
       "3  aaaammmiiiinnnn        amin\n",
       "4            aaahh          ah"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Text Normalization / Noise Removal\n",
    "indo_slang_word = pd.read_csv(\"./Dataset/TextNormalization/lexiconclean.csv\")\n",
    "\n",
    "display(indo_slang_word.shape)\n",
    "display(indo_slang_word.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def replace_slang_word(doc,slang_word):\n",
    "# #     for index in  range(0,len(doc)-1):\n",
    "# #         index_slang = slang_word.slang==doc[index]\n",
    "# #         formal = list(set(slang_word[index_slang].formal))\n",
    "# #         # replace slang word with formal word\n",
    "# #         if len(formal)>0:\n",
    "# #             doc[index] = formal[0]\n",
    "        \n",
    "# #     return doc\n",
    "# def replace_slang_word(doc, slang_word):\n",
    "#     for index in range(len(doc)):\n",
    "#         slang_match = slang_word[slang_word['slang'].isin([doc[index]])]\n",
    "#         if not slang_match.empty:\n",
    "#             formal = list(set(slang_match['formal']))\n",
    "#             if len(formal) > 0:\n",
    "#                 doc[index] = formal[0]\n",
    "#     return doc\n",
    "\n",
    "# prevent word to cut in the middle of word for example: 'kemarin' -> 'kemarin' not 'kemar' 'in'\n",
    "\n",
    "def replace_slang_word(doc, slang_word):\n",
    "    for index in range(len(doc)):\n",
    "        slang_match = slang_word[slang_word['slang'].isin([doc[index]])]\n",
    "        if not slang_match.empty:\n",
    "            formal = list(set(slang_match['formal']))\n",
    "            if len(formal) > 0:\n",
    "                doc[index] = formal[0]\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             comments  label\n",
      "0   [presiden, indonesia, perubahan, bangsa, indon...    1.0\n",
      "1                             [aldi, menang, putaran]    1.0\n",
      "2                 [calon, wapresnya, keluarga, pilih]    1.0\n",
      "3    [kalo, langsung, menang, tampa, berputar, putar]    1.0\n",
      "4          [kalo, indonesia, hancur, pilih, presiden]    0.0\n",
      "..                                                ...    ...\n",
      "59                     [semoga, allah, swt, menolong]    1.0\n",
      "60                 [petugas, partai, calon, presiden]    0.0\n",
      "61  [cinta, banget, umur, pemilihan, dukung, smpe,...    1.0\n",
      "62                             [coba, bambang, cocok]    1.0\n",
      "63                            [bagusnya, pasangannya]    1.0\n",
      "\n",
      "[64 rows x 2 columns]\n",
      "                                             comments  label\n",
      "0   [presiden, indonesia, perubahan, bangsa, indon...    1.0\n",
      "1                             [aldi, menang, putaran]    1.0\n",
      "2                 [calon, wapresnya, keluarga, pilih]    1.0\n",
      "3   [kalo , langsung, menang, tampa, berputar, putar]    1.0\n",
      "4         [kalo , indonesia, hancur, pilih, presiden]    0.0\n",
      "..                                                ...    ...\n",
      "59                     [semoga, allah, swt, menolong]    1.0\n",
      "60                 [petugas, partai, calon, presiden]    0.0\n",
      "61  [cinta, sekali , umur, pemilihan, dukung, samp...    1.0\n",
      "62                             [coba, bambang, cocok]    1.0\n",
      "63                            [bagusnya, pasangannya]    1.0\n",
      "\n",
      "[64 rows x 2 columns]\n",
      "['presiden', 'indonesia', 'perubahan', 'bangsa', 'indonesia', 'negara', 'indonesia', 'segi', 'ekonomi', 'segi', 'hukum']\n"
     ]
    }
   ],
   "source": [
    "print(secondary_dataset)\n",
    "# convert data frame to array\n",
    "# secondary_dataset['comments'] = secondary_dataset['comments'].apply(lambda x: np.array(x))\n",
    "normal_data = secondary_dataset\n",
    "normal_data = secondary_dataset['comments'].apply(replace_slang_word, slang_word=indo_slang_word)\n",
    "# append data to secondary_dataset\n",
    "print(secondary_dataset)\n",
    "\n",
    "#testing\n",
    "testing = secondary_dataset['comments'][0]\n",
    "print(testing)\n",
    "testing = replace_slang_word(testing, indo_slang_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_indo(doc):\n",
    "    return [stemmer.stem(word) for word in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['presiden',\n",
       " 'indonesia',\n",
       " 'perubahan',\n",
       " 'bangsa',\n",
       " 'indonesia',\n",
       " 'negara',\n",
       " 'indonesia',\n",
       " 'segi',\n",
       " 'ekonomi',\n",
       " 'segi',\n",
       " 'hukum']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[presiden, indonesia, ubah, bangsa, indonesia,...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[aldi, menang, putar]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[calon, wapresnya, keluarga, pilih]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[kalo, langsung, menang, tampa, putar, putar]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[kalo, indonesia, hancur, pilih, presiden]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>[moga, allah, swt, tolong]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>[tugas, partai, calon, presiden]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>[cinta, sekali, umur, pilih, dukung, sampai, s...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>[coba, bambang, cocok]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>[bagus, pasang]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comments  label\n",
       "0   [presiden, indonesia, ubah, bangsa, indonesia,...    1.0\n",
       "1                               [aldi, menang, putar]    1.0\n",
       "2                 [calon, wapresnya, keluarga, pilih]    1.0\n",
       "3       [kalo, langsung, menang, tampa, putar, putar]    1.0\n",
       "4          [kalo, indonesia, hancur, pilih, presiden]    0.0\n",
       "..                                                ...    ...\n",
       "59                         [moga, allah, swt, tolong]    1.0\n",
       "60                   [tugas, partai, calon, presiden]    0.0\n",
       "61  [cinta, sekali, umur, pilih, dukung, sampai, s...    1.0\n",
       "62                             [coba, bambang, cocok]    1.0\n",
       "63                                    [bagus, pasang]    1.0\n",
       "\n",
       "[64 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# stemming\n",
    "display(secondary_dataset['comments'][0])\n",
    "stemmed_data = secondary_dataset['comments'].apply(lemma_indo)\n",
    "# append stemmed data to secondary dataset, don't forget to reset index\n",
    "secondary_dataset['comments'] = stemmed_data\n",
    "secondary_dataset = secondary_dataset.reset_index(drop=True)\n",
    "display(secondary_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Minority Oversampling Technique (SMOTE)\n",
    "\n",
    "* this one need research (ivan)\n",
    "* Smote adalah sebuah tehnik yang digunakan terhadap data yang tidak seimbang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smote\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# clean empty row\n",
    "# Instantiate SMOTE\n",
    "smote = SMOTE(\n",
    "    sampling_strategy='minority',\n",
    "    random_state=42\n",
    ")\n",
    "# Assuming 'comments' is the name of the text column\n",
    "comments = secondary_dataset['comments']\n",
    "# Assuming 'label' is name of the target column\n",
    "y = secondary_dataset['label']\n",
    "\n",
    "\n",
    "# vectorizer tokenized comments\n",
    "# vectorizer = CountVectorizer()\n",
    "# TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# vectorizer = CountVectorizer()\n",
    "# X_tfidf = vectorizer.fit_transform(comments)\n",
    "joined_comments = pd.DataFrame(comments.apply(lambda x: ' '.join(x)))\n",
    "# display(joined_comments)\n",
    "# X_tfidf = vectorizer.fit_transform(joined_comments) # if we want to vectorize all comments in one row\n",
    "# fit vectorizer\n",
    "vectorizer.fit(joined_comments['comments'])\n",
    "# transform comments\n",
    "X_tfidf = vectorizer.transform(joined_comments['comments'])\n",
    "# vectorizer.fit(comments)\n",
    "# X_tfidf = vectorizer.transform(comments)\n",
    "# # display(X_tfidf)\n",
    "# # display(X_tfidf.shape)\n",
    "# Apply SMOTE to data\n",
    "X_smote, y_smote = smote.fit_resample(X_tfidf, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.54\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.33      0.40         6\n",
      "         1.0       0.56      0.71      0.63         7\n",
      "\n",
      "    accuracy                           0.54        13\n",
      "   macro avg       0.53      0.52      0.51        13\n",
      "weighted avg       0.53      0.54      0.52        13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# without SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate Multinomial Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "# Logistic Regression\n",
    "# Fit the classifier on the training data\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.65\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.60      0.63        10\n",
      "         1.0       0.64      0.70      0.67        10\n",
      "\n",
      "    accuracy                           0.65        20\n",
      "   macro avg       0.65      0.65      0.65        20\n",
      "weighted avg       0.65      0.65      0.65        20\n",
      "\n",
      "          negative  positive\n",
      "negative         6         4\n",
      "positive         3         7\n"
     ]
    }
   ],
   "source": [
    "# with SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_test = detik_anies_after['comments']\n",
    "y_test = detik_anies_after['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.3, random_state=42)\n",
    "\n",
    "# Instantiate Multinomial Naive Bayes classifier\n",
    "classifier = MultinomialNB(alpha=0.1)\n",
    "\n",
    "# Fit the classifier on the training data\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "\n",
    "# Confusion Matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the confusion matrix\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Transform to dataframe for easier plotting\n",
    "matrix_df = pd.DataFrame(matrix,\n",
    "                        index=['negative', 'positive'], \n",
    "                        columns=['negative', 'positive'])\n",
    "\n",
    "# print\n",
    "print(matrix_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1.0    10\n",
       "0.0    10\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters for MultinomialNB: {'alpha': 0.05, 'class_prior': [0.75, 0.25], 'fit_prior': True}\n",
      "Best accuracy for MultinomialNB: 0.7653846153846153\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# import kfold\n",
    "from sklearn.model_selection import KFold\n",
    "# Gradient Boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# All naive bayes\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Create k-Fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for combination of alpha and fit_prior\n",
    "model_param_grid = {\n",
    "    'alpha': [0.01, 0.05,0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 100.0],\n",
    "    'fit_prior': [True, False],\n",
    "    'class_prior': [None, [0.25, 0.75], [0.5, 0.5], [0.75, 0.25]]\n",
    "    }\n",
    "# model_param_grid = {\n",
    "#     'alpha': [0.01, 0.05,0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 100.0],\n",
    "#     'fit_prior': [True, False],\n",
    "#     }\n",
    "# for gaussian naive bayes\n",
    "\n",
    "model_grid_search = GridSearchCV(MultinomialNB(), model_param_grid, cv=kfold)\n",
    "model_grid_search.fit(X_smote, y_smote)\n",
    "# Print the best parameters and corresponding accuracy\n",
    "print(\"\\nBest parameters for MultinomialNB:\", model_grid_search.best_params_)\n",
    "print(\"Best accuracy for MultinomialNB:\", model_grid_search.best_score_)\n",
    "\n",
    "best_alpha = model_grid_search.best_params_['alpha']\n",
    "best_fit_prior = model_grid_search.best_estimator_.fit_prior\n",
    "\n",
    "display(best_alpha)\n",
    "display(best_fit_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation\n",
    "\n",
    "* this one need research (abhi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Accuracy = 0.7142857142857143 <== BEST RESULT\n",
      "Fold 2: Accuracy = 0.8571428571428571 <== BEST RESULT\n",
      "Fold 3: Accuracy = 0.7142857142857143\n",
      "Fold 4: Accuracy = 0.8571428571428571\n",
      "Fold 5: Accuracy = 0.5\n",
      "Fold 6: Accuracy = 0.3333333333333333\n",
      "Fold 7: Accuracy = 0.8333333333333334\n",
      "Fold 8: Accuracy = 0.6666666666666666\n",
      "Fold 9: Accuracy = 0.8333333333333334\n",
      "Fold 10: Accuracy = 0.8333333333333334\n",
      "\n",
      "Mean Accuracy: 0.7142857142857142\n",
      "Standard Deviation: 0.1663261828245046\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "# all forest algorithm\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "# # Define 10-fold cross-validation test harness\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# # Instantiate classifier \n",
    "classifier = MultinomialNB(alpha=best_alpha, fit_prior=best_fit_prior)\n",
    "\n",
    "cv_results = cross_val_score(classifier, X_smote, y_smote, cv=kfold)\n",
    "# # Print the results for each fold\n",
    "best_fold = 0\n",
    "best_accuracy = 0\n",
    "\n",
    "for i, accuracy in enumerate(cv_results):\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_fold = i\n",
    "        print(f\"Fold {i + 1}: Accuracy = {accuracy} <== BEST RESULT\")\n",
    "    else:\n",
    "        print(f\"Fold {i + 1}: Accuracy = {accuracy}\")\n",
    "\n",
    "# from best fold, we will get the best model\n",
    "# Instantiate classifier\n",
    "# Print the mean and standard deviation of the cross-validation results\n",
    "print(f\"\\nMean Accuracy: {cv_results.mean()}\")\n",
    "print(f\"Standard Deviation: {cv_results.std()}\")\n",
    "# # After k-folds, now we will use the best model to predict the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workload Distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "<!-- * kemungkinan Binomial Naive Bayes -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "classifier.fit(X_smote, y_smote)\n",
    "\n",
    "def prediction(data):    \n",
    "    tokenized = []\n",
    "    for i in range(0, len(data)):\n",
    "        # Check if the value is NaN or not a string\n",
    "        if pd.notna(data[i]) and isinstance(data[i], str):\n",
    "            tokenized.extend(word_tokenize(data[i]))\n",
    "    # Transform using the same vectorizer used during training\n",
    "    text = vectorizer.transform([\" \".join(tokenized)]).toarray()\n",
    "    # return classifier.predict(text)\n",
    "    return classifier.predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_and_save_data(X, prediction_function, output_path='./temp.csv'):\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    label = []\n",
    "    predictionLabel = []\n",
    "\n",
    "    for i in range(0, len(X)):\n",
    "        text = [X['comments'][i]]\n",
    "        pred = prediction_function(text)\n",
    "        predictionLabel.append(pred[0])\n",
    "        if pred == 1:\n",
    "            pos += 1\n",
    "        else:\n",
    "            neg += 1\n",
    "        label.append(pred[0])\n",
    "\n",
    "    X['label'] = label\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    X.to_csv(output_path, index=False)\n",
    "    print(\"Positive:\", pos)\n",
    "    print(\"Negative:\", neg)\n",
    "    # print percentage of positive and negative\n",
    "    print(\"Positive percentage:\", pos/len(X)*100)\n",
    "    print(\"Negative percentage:\", neg/len(X)*100)\n",
    "    return pos, neg, predictionLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aurel\\AppData\\Local\\Temp\\ipykernel_18348\\3654328601.py:84: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['comments'] = data['comments'].apply(clean_text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0.0    305\n",
      "1.0     48\n",
      "Name: count, dtype: int64\n",
      "Positive: 121\n",
      "Negative: 232\n",
      "Positive percentage: 34.27762039660057\n",
      "Negative percentage: 65.72237960339945\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.66      0.75       305\n",
      "         1.0       0.15      0.38      0.21        48\n",
      "\n",
      "    accuracy                           0.62       353\n",
      "   macro avg       0.51      0.52      0.48       353\n",
      "weighted avg       0.77      0.62      0.68       353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'comments' is the text data in your dataset\n",
    "# X = pd.read_csv('./Dataset/Detik/', on_bad_lines='skip')\n",
    "X = detik_anies_before\n",
    "# clean data\n",
    "X = X.dropna()\n",
    "X = remove_punct(X)\n",
    "# norm\n",
    "X['comments'] = X['comments'].apply(word_tokenize)\n",
    "X['comments'] = X['comments'].apply(replace_slang_word, slang_word=indo_slang_word)\n",
    "X['comments'] = X['comments'].apply(lemma_indo)\n",
    "# remove stopword\n",
    "\n",
    "# convert back to string\n",
    "X['comments'] = X['comments'].apply(lambda x: ' '.join(x))\n",
    "# X['comments'] = X['comments'].apply(stopword_removal)\n",
    "print(X['label'].value_counts())\n",
    "X['comments'] = X['comments'].astype(str)\n",
    "true_label = X['label']\n",
    "pos, neg, predictionLabel = process_and_save_data(X, prediction, './temp.csv')\n",
    "\n",
    "print(\"\\n\")\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(true_label, predictionLabel))\n",
    "\n",
    "# accuracy 75%\n",
    "# accuracy 76%\n",
    "# accuracy 74%\n",
    "# accuracy 79%\n",
    "# accuracy 78%\n",
    "# accuracy 80%\n",
    "# accuracy 69%\n",
    "# accuracy 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    72\n",
      "1    44\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    72\n",
      "1    44\n",
      "Name: count, dtype: int64\n",
      "Positive: 46\n",
      "Negative: 70\n",
      "Positive percentage: 39.6551724137931\n",
      "Negative percentage: 60.3448275862069\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.79      0.80        72\n",
      "           1       0.67      0.70      0.69        44\n",
      "\n",
      "    accuracy                           0.76       116\n",
      "   macro avg       0.74      0.75      0.75       116\n",
      "weighted avg       0.76      0.76      0.76       116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'comments' is the text data in your dataset\n",
    "# X = pd.read_csv('./Dataset/Detik/', on_bad_lines='skip')\n",
    "X = detik_ganjar_before\n",
    "# see how much true label\n",
    "print(X['label'].value_counts())\n",
    "# clean data\n",
    "X = X.dropna()\n",
    "X = remove_punct(X)\n",
    "# norm\n",
    "X['comments'] = X['comments'].apply(word_tokenize)\n",
    "X['comments'] = X['comments'].apply(replace_slang_word, slang_word=indo_slang_word)\n",
    "X['comments'] = X['comments'].apply(lemma_indo)\n",
    "# remove stopword\n",
    "\n",
    "# convert back to string\n",
    "X['comments'] = X['comments'].apply(lambda x: ' '.join(x))\n",
    "# X['comments'] = X['comments'].apply(stopword_removal)\n",
    "print(X['label'].value_counts())\n",
    "X['comments'] = X['comments'].astype(str)\n",
    "true_label = X['label']\n",
    "pos, neg, predictionLabel = process_and_save_data(X, prediction, './temp.csv')\n",
    "\n",
    "print(\"\\n\")\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(true_label, predictionLabel))\n",
    "\n",
    "# accuracy 87%\n",
    "# accuracy 90%\n",
    "# accuracy 89%\n",
    "# accuracy 84%\n",
    "# accuracy 83%\n",
    "# accuracy 80%\n",
    "# accuracy 84%\n",
    "# accuracy 82%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "1.0    32\n",
      "0.0    32\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "1.0    32\n",
      "0.0    32\n",
      "Name: count, dtype: int64\n",
      "Positive: 31\n",
      "Negative: 33\n",
      "Positive percentage: 48.4375\n",
      "Negative percentage: 51.5625\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      1.00      0.98        32\n",
      "         1.0       1.00      0.97      0.98        32\n",
      "\n",
      "    accuracy                           0.98        64\n",
      "   macro avg       0.98      0.98      0.98        64\n",
      "weighted avg       0.98      0.98      0.98        64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = Training_dataset\n",
    "# see how much true label\n",
    "print(X['label'].value_counts())\n",
    "# clean data\n",
    "X = X.dropna()\n",
    "X = remove_punct(X)\n",
    "# norm\n",
    "X['comments'] = X['comments'].apply(word_tokenize)\n",
    "X['comments'] = X['comments'].apply(replace_slang_word, slang_word=indo_slang_word)\n",
    "X['comments'] = X['comments'].apply(lemma_indo)\n",
    "# remove stopword\n",
    "\n",
    "# convert back to string\n",
    "X['comments'] = X['comments'].apply(lambda x: ' '.join(x))\n",
    "# X['comments'] = X['comments'].apply(stopword_removal)\n",
    "print(X['label'].value_counts())\n",
    "X['comments'] = X['comments'].astype(str)\n",
    "true_label = X['label']\n",
    "pos, neg, predictionLabel = process_and_save_data(X, prediction, './temp.csv')\n",
    "\n",
    "print(\"\\n\")\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(true_label, predictionLabel))\n",
    "\n",
    "# accuracy 90%\n",
    "# accuracy 94%\n",
    "# accuracy 96%\n",
    "# accuracy 96%\n",
    "# accuracy 96%\n",
    "# accuracy 97%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0.0    319\n",
      "1.0     51\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aurel\\AppData\\Local\\Temp\\ipykernel_18348\\3654328601.py:84: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['comments'] = data['comments'].apply(clean_text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0.0    305\n",
      "1.0     48\n",
      "Name: count, dtype: int64\n",
      "Positive: 121\n",
      "Negative: 232\n",
      "Positive percentage: 34.27762039660057\n",
      "Negative percentage: 65.72237960339945\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.66      0.75       305\n",
      "         1.0       0.15      0.38      0.21        48\n",
      "\n",
      "    accuracy                           0.62       353\n",
      "   macro avg       0.51      0.52      0.48       353\n",
      "weighted avg       0.77      0.62      0.68       353\n",
      "\n",
      "          negative  positive\n",
      "negative       202       103\n",
      "positive        30        18\n"
     ]
    }
   ],
   "source": [
    "X = detik_anies_after\n",
    "# see how much true label\n",
    "print(X['label'].value_counts())\n",
    "# clean data\n",
    "X = X.dropna()\n",
    "X = remove_punct(X)\n",
    "# norm\n",
    "X['comments'] = X['comments'].apply(word_tokenize)\n",
    "X['comments'] = X['comments'].apply(replace_slang_word, slang_word=indo_slang_word)\n",
    "X['comments'] = X['comments'].apply(lemma_indo)\n",
    "# remove stopword\n",
    "\n",
    "# convert back to string\n",
    "X['comments'] = X['comments'].apply(lambda x: ' '.join(x))\n",
    "# X['comments'] = X['comments'].apply(stopword_removal)\n",
    "print(X['label'].value_counts())\n",
    "X['comments'] = X['comments'].astype(str)\n",
    "true_label = X['label']\n",
    "pos, neg, predictionLabel = process_and_save_data(X, prediction, './temp.csv')\n",
    "\n",
    "print(\"\\n\")\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(true_label, predictionLabel))\n",
    "\n",
    "# accuracy 76%\n",
    "# accuracy 79%\n",
    "# accuracy 78%\n",
    "# accuracy 80%\n",
    "# accuracy 69%\n",
    "# accuracy 74%\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the confusion matrix\n",
    "matrix = confusion_matrix(true_label, predictionLabel)\n",
    "\n",
    "# Transform to dataframe for easier plotting\n",
    "matrix_df = pd.DataFrame(matrix,\n",
    "                        index=['negative', 'positive'], \n",
    "                        columns=['negative', 'positive'])\n",
    "\n",
    "# print\n",
    "print(matrix_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "1.0    92\n",
      "0.0     6\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aurel\\AppData\\Local\\Temp\\ipykernel_18348\\3654328601.py:84: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['comments'] = data['comments'].apply(clean_text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "1.0    82\n",
      "0.0     5\n",
      "Name: count, dtype: int64\n",
      "Positive: 40\n",
      "Negative: 47\n",
      "Positive percentage: 45.97701149425287\n",
      "Negative percentage: 54.02298850574713\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.06      0.60      0.12         5\n",
      "         1.0       0.95      0.46      0.62        82\n",
      "\n",
      "    accuracy                           0.47        87\n",
      "   macro avg       0.51      0.53      0.37        87\n",
      "weighted avg       0.90      0.47      0.59        87\n",
      "\n",
      "          negative  positive\n",
      "negative         3         2\n",
      "positive        44        38\n"
     ]
    }
   ],
   "source": [
    "X = ganjar_after\n",
    "# see how much true label\n",
    "print(X['label'].value_counts())\n",
    "# clean data\n",
    "X = X.dropna()\n",
    "X = remove_punct(X)\n",
    "# norm\n",
    "X['comments'] = X['comments'].apply(word_tokenize)\n",
    "X['comments'] = X['comments'].apply(replace_slang_word, slang_word=indo_slang_word)\n",
    "X['comments'] = X['comments'].apply(lemma_indo)\n",
    "# remove stopword\n",
    "\n",
    "# convert back to string\n",
    "X['comments'] = X['comments'].apply(lambda x: ' '.join(x))\n",
    "# X['comments'] = X['comments'].apply(stopword_removal)\n",
    "print(X['label'].value_counts())\n",
    "X['comments'] = X['comments'].astype(str)\n",
    "true_label = X['label']\n",
    "pos, neg, predictionLabel = process_and_save_data(X, prediction, './temp.csv')\n",
    "\n",
    "print(\"\\n\")\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(true_label, predictionLabel))\n",
    "\n",
    "# accuracy 0.71%\n",
    "# accuracy 0.68%\n",
    "# accuracy 0.39%\n",
    "# accuracy 0.64%\n",
    "\n",
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the confusion matrix\n",
    "matrix = confusion_matrix(true_label, predictionLabel)\n",
    "\n",
    "# Transform to dataframe for easier plotting\n",
    "matrix_df = pd.DataFrame(matrix,\n",
    "                        index=['negative', 'positive'], \n",
    "                        columns=['negative', 'positive'])\n",
    "\n",
    "# print\n",
    "print(matrix_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "* this one need research (pat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis the data from primary data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modal Comprarison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
