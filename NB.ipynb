{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis dari Instagram comments calon Presiden 2024 sebelum dan sesudah deklrasi calon wakil presiden menggunakan Naive Bayes\n",
    "\n",
    "\n",
    "## Tujuan : mengetahui perbedaan sentiment dari komentar instagram sebelum dan sesudah deklarasi calon wakil presiden\n",
    "\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "    Aurelius Ivan Wijaya (00000054769)\n",
    "    </li>\n",
    "    <li>\n",
    "    Rajendra Abhinaya (00000060445)\n",
    "    </li>\n",
    "    <li>\n",
    "    Maecyntha Irelynn Tantra (00000055038)\n",
    "    </li>\n",
    "    <li>\n",
    "    Patricia theodora (00000054093)\n",
    "    </li>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "apa sih yang sebenernya kita cari?\n",
    "* web scrapping algoritm untuk data primer (ivan)\n",
    "* labeling (pat)\n",
    "* data sekunder (mae)\n",
    "* stopword library indonesia (abhi)\n",
    "* cari jurnal referensi yang sudah, sebagai literature review (all, min 4 per person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset    \n",
    "# [ Primary Dataset ]\n",
    "# anies_before = pd.read_csv('./Dataset/Anies/anies_before.csv', on_bad_lines='skip')\n",
    "# anies_after = pd.DataFrame(pd.read_csv('./Dataset/Anies/anies_after.csv', on_bad_lines=\"skip\"))\n",
    "ganjar_before = pd.DataFrame(pd.read_csv('./Dataset/Ganjar/ganjar_before.csv', on_bad_lines=\"skip\")) # get only 100 data\n",
    "ganjar_before = ganjar_before[:100]\n",
    "ganjar_after = pd.DataFrame(pd.read_csv('./Dataset/Ganjar/ganjar_after.csv', on_bad_lines=\"skip\"))\n",
    "ganjar_after = ganjar_after[:100]\n",
    "prabowo_before = pd.DataFrame(pd.read_csv('./Dataset/Prabowo/prabowo_before.csv', on_bad_lines=\"skip\"))\n",
    "prabowo_before = prabowo_before[:100]\n",
    "prabowo_after = pd.DataFrame(pd.read_csv('./Dataset/Prabowo/prabowo_after.csv', on_bad_lines=\"skip\"))\n",
    "prabowo_after = prabowo_after[:100]\n",
    "# prabowo_before = pd.DataFrame(pd.read_csv('./Dataset/Detik/detik_prabowo_before.csv', on_bad_lines=\"skip\"))\n",
    "# prabowo_after = pd.DataFrame(pd.read_csv('./Dataset/Detik/detik_prabowo_after.csv', on_bad_lines=\"skip\"   ))\n",
    "detik_anies_before = pd.read_csv('./Dataset/Detik/detik_anies_before.csv', on_bad_lines='skip')\n",
    "detik_anies_after = pd.DataFrame(pd.read_csv('./Dataset/Detik/detik_anies_before.csv', on_bad_lines=\"skip\"))\n",
    "detik_ganjar_before = pd.DataFrame(pd.read_csv('./Dataset/Detik/detik_ganjar_before.csv', on_bad_lines=\"skip\"))\n",
    "detik_ganjar_after = pd.DataFrame(pd.read_csv('./Dataset/Detik/detik_ganjar_after.csv', on_bad_lines=\"skip\"))\n",
    "detik_prabowo_before = pd.DataFrame(pd.read_csv('./Dataset/Detik/detik_prabowo_before.csv', on_bad_lines=\"skip\"))\n",
    "detik_prabowo_after = pd.DataFrame(pd.read_csv('./Dataset/Detik/detik_prabowo_after.csv', on_bad_lines=\"skip\"   ))\n",
    "# [ Secondary Dataset ]\n",
    "instagram_cyber_comments = pd.read_csv('./Dataset/dataset_komentar_instagram_cyberbullying.csv')\n",
    "tweet_tv = pd.read_csv('./Dataset/dataset_tweet_sentimen_tayangan_tv.csv')\n",
    "tweet_pilkada = pd.read_csv('./Dataset/dataset_tweet_sentiment_pilkada_DKI_2017.csv')\n",
    "tweet_opini_film = pd.read_csv('./Dataset/dataset_tweet_sentiment_opini_film.csv')\n",
    "tweet_cellular = pd.read_csv('./Dataset/dataset_tweet_sentiment_cellular_service_provider.csv')\n",
    "prastyo_sentiment = pd.read_csv('./Dataset/prastyo-sentiment_all.csv')\n",
    "sentiment_twitter_indonesia = pd.read_csv('./Dataset/Indonesian Sentiment Twitter Dataset Labeled.csv', sep='\\t')\n",
    "# sentiment_shopee = pd.read_csv('./Dataset/dataset_shopee2.csv', on_bad_lines=\"skip\")\n",
    "import chardet\n",
    "\n",
    "# Detect encoding\n",
    "with open('./Dataset/dataset_shopee2.csv', 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "# Use the detected encoding when reading the CSV file\n",
    "sentiment_shopee = pd.read_csv('./Dataset/dataset_shopee2.csv', on_bad_lines=\"skip\", encoding=result['encoding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def balance_dataset(data, label_column, max_samples=None):\n",
    "\n",
    "    # Determine the minimum number of samples for any label\n",
    "    min_samples = data[label_column].value_counts().min()\n",
    "\n",
    "    # Randomly select min_samples samples for each label\n",
    "    balanced_subset = pd.concat([group.sample(min_samples) for _, group in data.groupby(label_column)])\n",
    "\n",
    "    # Shuffle the balanced subset to mix the labels\n",
    "    balanced_subset = balanced_subset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Optionally only keep a maximum number of samples\n",
    "    if max_samples is not None:\n",
    "        balanced_subset = balanced_subset[:max_samples]\n",
    "    \n",
    "    return balanced_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: positive, 0: negative \n",
    "\n",
    "# data integration\n",
    "# change label name to 'comments'\n",
    "instagram_cyber_comments.rename(columns={'Instagram Comment Text': 'comments'}, inplace=True)\n",
    "# mapping sentiment\n",
    "instagram_cyber_comments['Sentiment'] = instagram_cyber_comments['Sentiment'].map({'positive': 1, 'negative': 0})\n",
    "instagram_cyber_comments['label'] = instagram_cyber_comments['Sentiment'].astype(int)\n",
    "# drop unused columns\n",
    "instagram_cyber_comments.drop(columns=['Id', 'Sentiment'], inplace=True)\n",
    "# display(instagram_cyber_comments.head())\n",
    "#change data type to string to ensure all data type is string\n",
    "instagram_cyber_comments['comments'] = instagram_cyber_comments['comments'].astype(str)\n",
    "\n",
    "# tweet_tv\n",
    "# change label name to 'comments'\n",
    "tweet_tv.rename(columns={'Tweet': 'comments'}, inplace=True)\n",
    "# mapping sentiment\n",
    "tweet_tv['Sentiment'] = tweet_tv['Sentiment'].map({'positive': 1, 'negative': 0})\n",
    "tweet_tv['label'] = tweet_tv['Sentiment'].astype(int)\n",
    "tweet_tv.rename(columns={'Text Tweet': 'comments'}, inplace=True)\n",
    "tweet_tv.drop(columns=['Id', 'Sentiment', \"Jumlah Retweet\", \"Acara TV\"], inplace=True)\n",
    "#change data type to string to ensure all data type is string\n",
    "tweet_tv['comments'] = tweet_tv['comments'].astype(str)\n",
    "\n",
    "# tweet_pilkada\n",
    "# tweet_pilkada add new column 'label'\n",
    "# tweet_pilkada['label'] = tweet_pilkada['Sentiment'].map({'positive': 1, 'negative': 0}).astype(int)\n",
    "tweet_pilkada['Sentiment'] = tweet_pilkada['Sentiment'].map({'positive': 1, 'negative': 0}).astype(int)\n",
    "# tweet_pilkada['label'] = tweet_pilkada['Sentiment'].astype(int)\n",
    "tweet_pilkada = tweet_pilkada[['Sentiment', 'Text Tweet']]\n",
    "tweet_pilkada.rename(columns={'Text Tweet': 'comments'}, inplace=True)\n",
    "tweet_pilkada.rename(columns={'Sentiment': 'label'}, inplace=True)\n",
    "#change data type to string to ensure all data type is string\n",
    "tweet_pilkada['comments'] = tweet_pilkada['comments'].astype(str)\n",
    "\n",
    "# tweet_opini_film\n",
    "# change label name to 'comments'\n",
    "tweet_opini_film.rename(columns={'Tweet': 'comments'}, inplace=True)\n",
    "# mapping sentiment\n",
    "tweet_opini_film['Sentiment'] = tweet_opini_film['Sentiment'].map({'positive': 1, 'negative': 0})\n",
    "tweet_opini_film['label'] = tweet_opini_film['Sentiment'].astype(int)\n",
    "tweet_opini_film.drop(columns=['Id', 'Sentiment'], inplace=True)\n",
    "tweet_opini_film.rename(columns={'Text Tweet': 'comments'}, inplace=True)\n",
    "#change data type to string to ensure all data type is string\n",
    "tweet_opini_film['comments'] = tweet_opini_film['comments'].astype(str)\n",
    "\n",
    "# tweet_cellular\n",
    "# change label name to 'comments'\n",
    "tweet_cellular.rename(columns={'Tweet': 'comments'}, inplace=True)\n",
    "# mapping sentiment\n",
    "tweet_cellular['Sentiment'] = tweet_cellular['Sentiment'].map({'positive': 1, 'negative': 0})\n",
    "tweet_cellular['label'] = tweet_cellular['Sentiment'].astype(int)\n",
    "tweet_cellular.drop(columns=['Id', 'Sentiment'], inplace=True)\n",
    "tweet_cellular.rename(columns={'Text Tweet': 'comments'}, inplace=True)\n",
    "\n",
    "# prastyo_sentiment\n",
    "map = {'pos': 1, 'neg': 0, 'neu': 1}\n",
    "prastyo_sentiment['label'] = prastyo_sentiment['label'].map(map)\n",
    "\n",
    "\n",
    "# sentiment twitter indonesia\n",
    "# change label name to 'comments'\n",
    "sentiment_twitter_indonesia.rename(columns={'Tweet': 'comments'}, inplace=True)\n",
    "# mapping sentiment\n",
    "# sentiment_twitter_indonesia['sentimen'] = sentiment_twitter_indonesia['sentimen'].map({'positive': 1, 'negative': 0})\n",
    "sentiment_twitter_indonesia['label'] = sentiment_twitter_indonesia['sentimen'].astype(int)\n",
    "\n",
    "\n",
    "# shoppe\n",
    "# change label name to 'comments'\n",
    "sentiment_shopee.rename(columns={'Review': 'comments'}, inplace=True)\n",
    "# mapping sentiment\n",
    "sentiment_shopee['SENTIMEN'] = sentiment_shopee['SENTIMEN'].map({'POSITIF': 1, 'NEGATIF': 0})\n",
    "sentiment_shopee['label'] = sentiment_shopee['SENTIMEN'].astype(int)\n",
    "sentiment_shopee.drop(columns=['SENTIMEN'], inplace=True)\n",
    "\n",
    "\n",
    "detik_anies_after = balance_dataset(detik_anies_after, 'label')\n",
    "detik_anies_before = balance_dataset(detik_anies_before, 'label')\n",
    "# detik_ganjar_before = balance_dataset(detik_ganjar_before, 'label', max_samples=50)\n",
    "# detik_ganjar_after = balance_dataset(detik_ganjar_after, 'label', max_samples=50)\n",
    "\n",
    "secondary_dataset = pd.concat([\n",
    "    # anies_before, # if we train with anies_before, the accuracy will be decrease\n",
    "    # anies_after,\n",
    "    # detik_anies_after,\n",
    "    detik_anies_before,\n",
    "    detik_ganjar_after,\n",
    "    detik_ganjar_before,\n",
    "    # sentiment_twitter_indonesia,\n",
    "    # tweet_opini_film\n",
    "    tweet_pilkada,\n",
    "    instagram_cyber_comments\n",
    "    ], ignore_index=True)\n",
    "# only pick comments and label column\n",
    "secondary_dataset = secondary_dataset[['comments', 'label']]\n",
    "#change data type to string to ensure all data type is string\n",
    "secondary_dataset['comments'] = secondary_dataset['comments'].astype(str)\n",
    "# if there is column 'Unnamed: 0' in secondary dataset, drop it\n",
    "if 'Unnamed: 0' in secondary_dataset.columns:\n",
    "    secondary_dataset.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Proccessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1608 entries, 0 to 1607\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   comments  1608 non-null   object \n",
      " 1   label     1608 non-null   float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 25.3+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apa yg omongan loe udah jalan waktu jadi gaben...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jika jalan hidupmu sulit n kacau, coba rubah n...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sorat sorot sorat sorot.. koyo²o iso mbenahi a...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ente emang paling pinter mengolah kata</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Woi dp nol apartemen lu gimana kabare</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            comments  label\n",
       "0  apa yg omongan loe udah jalan waktu jadi gaben...    0.0\n",
       "1  Jika jalan hidupmu sulit n kacau, coba rubah n...    0.0\n",
       "2  Sorat sorot sorat sorot.. koyo²o iso mbenahi a...    0.0\n",
       "3             Ente emang paling pinter mengolah kata    0.0\n",
       "4              Woi dp nol apartemen lu gimana kabare    0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(secondary_dataset.info())\n",
    "display(secondary_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1608 entries, 0 to 1607\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   comments  1600 non-null   object \n",
      " 1   label     1608 non-null   float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 25.3+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apa yg omongan loe udah jalan waktu jadi gaben...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jika jalan hidupmu sulit n kacau, coba rubah n...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sorat sorot sorat sorot.. koyo²o iso mbenahi a...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ente emang paling pinter mengolah kata</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Woi dp nol apartemen lu gimana kabare</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            comments  label\n",
       "0  apa yg omongan loe udah jalan waktu jadi gaben...    0.0\n",
       "1  Jika jalan hidupmu sulit n kacau, coba rubah n...    0.0\n",
       "2  Sorat sorot sorat sorot.. koyo²o iso mbenahi a...    0.0\n",
       "3             Ente emang paling pinter mengolah kata    0.0\n",
       "4              Woi dp nol apartemen lu gimana kabare    0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def handleMissingValue(df):\n",
    "    df = df.dropna()\n",
    "    df = df.drop_duplicates()\n",
    "    df = df[df['comments'] != '']\n",
    "    # remove 'NaN' value\n",
    "    if('NaN' in df['comments']):\n",
    "        df = df[df['comments'] != 'NaN']\n",
    "    df = df[df['comments'].notna()]\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df[['comments']] \n",
    "    return df\n",
    "secondary_dataset['comments'] = handleMissingValue(secondary_dataset)\n",
    "display(secondary_dataset.info())\n",
    "display(secondary_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comments    8\n",
      "label       0\n",
      "dtype: int64\n",
      "comments    0\n",
      "label       0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apa yg omongan loe udah jalan waktu jadi gaben...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jika jalan hidupmu sulit n kacau, coba rubah n...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sorat sorot sorat sorot.. koyo²o iso mbenahi a...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ente emang paling pinter mengolah kata</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Woi dp nol apartemen lu gimana kabare</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>Bangga sama suami yg selalu ingat istri disela...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>Apaoun pekerjaannya yg penting halal u tuk men...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>Gojek itu mayoritas pegangguran yang lama gak ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>&lt;USERNAME&gt; aslinya cantik dan ayu loh mbak kr...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>&lt;USERNAME&gt; suami saya seumuran sama saya mba,...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               comments  label\n",
       "0     apa yg omongan loe udah jalan waktu jadi gaben...    0.0\n",
       "1     Jika jalan hidupmu sulit n kacau, coba rubah n...    0.0\n",
       "2     Sorat sorot sorat sorot.. koyo²o iso mbenahi a...    0.0\n",
       "3                Ente emang paling pinter mengolah kata    0.0\n",
       "4                 Woi dp nol apartemen lu gimana kabare    0.0\n",
       "...                                                 ...    ...\n",
       "1595  Bangga sama suami yg selalu ingat istri disela...    1.0\n",
       "1596  Apaoun pekerjaannya yg penting halal u tuk men...    1.0\n",
       "1597  Gojek itu mayoritas pegangguran yang lama gak ...    1.0\n",
       "1598   <USERNAME> aslinya cantik dan ayu loh mbak kr...    1.0\n",
       "1599   <USERNAME> suami saya seumuran sama saya mba,...    1.0\n",
       "\n",
       "[1600 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(secondary_dataset.isnull().sum())\n",
    "# delete duplicate data and missing value and null value\n",
    "secondary_dataset = secondary_dataset.dropna()\n",
    "secondary_dataset = secondary_dataset.drop_duplicates()\n",
    "secondary_dataset = secondary_dataset[secondary_dataset['comments'] != '']\n",
    "# remove 'NaN' value\n",
    "if('NaN' in secondary_dataset['comments']):\n",
    "    secondary_dataset = secondary_dataset[secondary_dataset['comments'] != 'NaN']\n",
    "secondary_dataset = secondary_dataset[secondary_dataset['comments'].notna()]\n",
    "secondary_dataset = secondary_dataset.reset_index(drop=True)\n",
    "# secondary_dataset = secondary_dataset[['comments']]\n",
    "print(secondary_dataset.isnull().sum())\n",
    "\n",
    "display(secondary_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Precompile regex patterns\n",
    "re_long_word = re.compile(r'\\b\\w{20,}\\b')\n",
    "re_non_ascii = re.compile(r'[^\\x00-\\x7F]+')\n",
    "# re_punctuation = re.compile(f\"[{re.escape(string.punctuation)}]\")\n",
    "re_single_char = re.compile(r'\\s+[a-zA-Z]\\s+')\n",
    "re_numbers = re.compile(r'\\d+')\n",
    "re_tags = re.compile(\"&lt;/?.*?&gt;\")\n",
    "re_special_chars_digits = re.compile(\"(\\\\d|\\\\W)+\")\n",
    "re_repeated_chars = re.compile(r'(.)\\1+')\n",
    "\n",
    "import re\n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)\n",
    "\n",
    "def clean_text(comment):\n",
    "    comment = str(comment)\n",
    "    comment = comment.lower() # Case Folding to lowercase\n",
    "\n",
    "    # Remove non-ascii words and characters\n",
    "    comment = re_non_ascii.sub(' ', comment) # remove non-ascii characters\n",
    "\n",
    "    # Remove all links\n",
    "    comment = re.sub(r\"http\\S+\", \"\", comment) # remove all links\n",
    "\n",
    "    # Delete all emoticons from text\n",
    "    comment = re.sub(':[^ ]+:', '', comment) # remove emoticons\n",
    "    comment = remove_emojis(comment)\n",
    "    # remove all word that start with < and end with >\n",
    "    comment = re.sub('<[^>]*>', '', comment)\n",
    "    # Remove long words\n",
    "    comment = re_long_word.sub('', comment) # remove long words\n",
    "\n",
    "    # if length of comment is more than 7 and ended with \"ny\" convert it to \"nya\"\n",
    "    if(len(comment) > 7 and comment[-2:] == 'ny'):\n",
    "        comment = comment[:-2] + 'nya'\n",
    "    # Remove single characters\n",
    "    comment = re_single_char.sub(' ', comment) # remove single characters\n",
    "\n",
    "    # Remove numbers\n",
    "    comment = re_numbers.sub('', comment)\n",
    "\n",
    "    # Remove word that contains tags\n",
    "    comment = re_tags.sub('', comment)\n",
    "\n",
    "    # Remove special characters and digits\n",
    "    comment = re_special_chars_digits.sub(\" \", comment) \n",
    "\n",
    "    # If a letter is repeated more than 2 times, replace it with 1 time\n",
    "    comment = re_repeated_chars.sub(r'\\1\\1', comment)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    comment = re.sub(r'\\s+', ' ', comment)\n",
    "\n",
    "    return comment.strip()\n",
    "\n",
    "def remove_punct(data):\n",
    "    data['comments'] = data['comments'].apply(clean_text)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comments    0\n",
      "label       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Call the function\n",
    "secondary_dataset.to_csv('./dirty_data.csv', index=False)\n",
    "cleaned_data = remove_punct(secondary_dataset)\n",
    "secondary_dataset['comments'] = cleaned_data['comments']\n",
    "\n",
    "secondary_dataset = secondary_dataset[secondary_dataset['comments'] != '']\n",
    "# remove 'NaN' value\n",
    "if('NaN' in secondary_dataset['comments']):\n",
    "    secondary_dataset = secondary_dataset[secondary_dataset['comments'] != 'NaN']\n",
    "secondary_dataset = secondary_dataset[secondary_dataset['comments'].notna()]\n",
    "secondary_dataset = secondary_dataset.reset_index(drop=True)\n",
    "\n",
    "# secondary_dataset = secondary_dataset[['comments']]\n",
    "print(secondary_dataset.isnull().sum())\n",
    "# Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apa yg omongan loe udah jalan waktu jadi gaben...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jika jalan hidupmu sulit kacau coba rubah nama...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sorat sorot sorat sorot koyo iso mbenahi ae nd...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ente emang paling pinter mengolah kata</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>woi dp nol apartemen lu gimana kabare</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1583</th>\n",
       "      <td>bangga sama suami yg selalu ingat istri disela...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1584</th>\n",
       "      <td>apaoun pekerjaannya yg penting halal tuk menaf...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>gojek itu mayoritas pegangguran yang lama gak ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>aslinya cantik dan ayu loh mbak krn aku sudah ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587</th>\n",
       "      <td>suami saya seumuran sama saya mba malah tuaan ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1588 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               comments  label\n",
       "0     apa yg omongan loe udah jalan waktu jadi gaben...    0.0\n",
       "1     jika jalan hidupmu sulit kacau coba rubah nama...    0.0\n",
       "2     sorat sorot sorat sorot koyo iso mbenahi ae nd...    0.0\n",
       "3                ente emang paling pinter mengolah kata    0.0\n",
       "4                 woi dp nol apartemen lu gimana kabare    0.0\n",
       "...                                                 ...    ...\n",
       "1583  bangga sama suami yg selalu ingat istri disela...    1.0\n",
       "1584  apaoun pekerjaannya yg penting halal tuk menaf...    1.0\n",
       "1585  gojek itu mayoritas pegangguran yang lama gak ...    1.0\n",
       "1586  aslinya cantik dan ayu loh mbak krn aku sudah ...    1.0\n",
       "1587  suami saya seumuran sama saya mba malah tuaan ...    1.0\n",
       "\n",
       "[1588 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(secondary_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apa yg omongan loe udah jalan waktu jadi gaben...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jika jalan hidupmu sulit kacau coba rubah nama...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sorat sorot sorat sorot koyo iso mbenahi ae nd...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ente emang paling pinter mengolah kata</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>woi dp nol apartemen lu gimana kabare</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1583</th>\n",
       "      <td>bangga sama suami yg selalu ingat istri disela...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1584</th>\n",
       "      <td>apaoun pekerjaannya yg penting halal tuk menaf...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>gojek itu mayoritas pegangguran yang lama gak ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>aslinya cantik dan ayu loh mbak krn aku sudah ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587</th>\n",
       "      <td>suami saya seumuran sama saya mba malah tuaan ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1588 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               comments  label\n",
       "0     apa yg omongan loe udah jalan waktu jadi gaben...    0.0\n",
       "1     jika jalan hidupmu sulit kacau coba rubah nama...    0.0\n",
       "2     sorat sorot sorat sorot koyo iso mbenahi ae nd...    0.0\n",
       "3                ente emang paling pinter mengolah kata    0.0\n",
       "4                 woi dp nol apartemen lu gimana kabare    0.0\n",
       "...                                                 ...    ...\n",
       "1583  bangga sama suami yg selalu ingat istri disela...    1.0\n",
       "1584  apaoun pekerjaannya yg penting halal tuk menaf...    1.0\n",
       "1585  gojek itu mayoritas pegangguran yang lama gak ...    1.0\n",
       "1586  aslinya cantik dan ayu loh mbak krn aku sudah ...    1.0\n",
       "1587  suami saya seumuran sama saya mba malah tuaan ...    1.0\n",
       "\n",
       "[1588 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "display(secondary_dataset)\n",
    "# Stopword removal\n",
    "\n",
    "stopword_bahasa = pd.read_csv('./Dataset/TextNormalization/stopwordbahasa.csv', header=None)\n",
    "\n",
    "def stopword_removal(data):\n",
    "    for i in range(len(data)):\n",
    "        data = data.replace(stopword_bahasa[0][i], '')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yg omongan loe udah jalan waktu ji gabener dk...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jika jalan hidupmu sulit kacau coba rubah nama...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sorat sorot sorat sorot koyo iso mbenahi ae nd...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ente emang paling pinter mengolah kata</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>woi dp nol rtemen lu gimana kabare</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1583</th>\n",
       "      <td>bangga sama suami yg selalu ingat istri disela...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1584</th>\n",
       "      <td>oun pekerjaannya yg penting halal tuk menafkah...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>gojek itu mayoritas pegangguran yang lama gak ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>aslinya cantik  ayu loh mbak krn  sudah bertem...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587</th>\n",
       "      <td>suami saya seumuran sama saya mba malah tuaan ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1588 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               comments  label\n",
       "0      yg omongan loe udah jalan waktu ji gabener dk...    0.0\n",
       "1     jika jalan hidupmu sulit kacau coba rubah nama...    0.0\n",
       "2     sorat sorot sorat sorot koyo iso mbenahi ae nd...    0.0\n",
       "3                ente emang paling pinter mengolah kata    0.0\n",
       "4                    woi dp nol rtemen lu gimana kabare    0.0\n",
       "...                                                 ...    ...\n",
       "1583  bangga sama suami yg selalu ingat istri disela...    1.0\n",
       "1584  oun pekerjaannya yg penting halal tuk menafkah...    1.0\n",
       "1585  gojek itu mayoritas pegangguran yang lama gak ...    1.0\n",
       "1586  aslinya cantik  ayu loh mbak krn  sudah bertem...    1.0\n",
       "1587  suami saya seumuran sama saya mba malah tuaan ...    1.0\n",
       "\n",
       "[1588 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yg omongan loe udah jalan waktu ji gabener dk...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jika jalan hidupmu sulit kacau coba rubah nama...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sorat sorot sorat sorot koyo iso mbenahi ae nd...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ente emang paling pinter mengolah kata</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>woi dp nol rtemen lu gimana kabare</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578</th>\n",
       "      <td>bangga sama suami yg selalu ingat istri disela...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579</th>\n",
       "      <td>oun pekerjaannya yg penting halal tuk menafkah...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1580</th>\n",
       "      <td>gojek itu mayoritas pegangguran yang lama gak ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581</th>\n",
       "      <td>aslinya cantik  ayu loh mbak krn  sudah bertem...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>suami saya seumuran sama saya mba malah tuaan ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1583 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               comments  label\n",
       "0      yg omongan loe udah jalan waktu ji gabener dk...    0.0\n",
       "1     jika jalan hidupmu sulit kacau coba rubah nama...    0.0\n",
       "2     sorat sorot sorat sorot koyo iso mbenahi ae nd...    0.0\n",
       "3                ente emang paling pinter mengolah kata    0.0\n",
       "4                    woi dp nol rtemen lu gimana kabare    0.0\n",
       "...                                                 ...    ...\n",
       "1578  bangga sama suami yg selalu ingat istri disela...    1.0\n",
       "1579  oun pekerjaannya yg penting halal tuk menafkah...    1.0\n",
       "1580  gojek itu mayoritas pegangguran yang lama gak ...    1.0\n",
       "1581  aslinya cantik  ayu loh mbak krn  sudah bertem...    1.0\n",
       "1582  suami saya seumuran sama saya mba malah tuaan ...    1.0\n",
       "\n",
       "[1583 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Call the function\n",
    "# cleaned_data = stopword_removal(secondary_dataset)\n",
    "# convert comments to string\n",
    "# secondary_dataset['comments'] = secondary_dataset['comments'].astype(str)\n",
    "\n",
    "#check null value\n",
    "secondary_dataset['comments'] = secondary_dataset['comments'].apply(stopword_removal)\n",
    "# secondary_dataset['comments'] = secondary_dataset['comments'].apply(lambda x: stopword_removal(x))\n",
    "display(secondary_dataset)\n",
    "\n",
    "# handle null, missing value, and duplicate data\n",
    "secondary_dataset = secondary_dataset.dropna()\n",
    "secondary_dataset = secondary_dataset.drop_duplicates()\n",
    "secondary_dataset = secondary_dataset[secondary_dataset['comments'] != '']\n",
    "# remove 'NaN' value\n",
    "if('NaN' in secondary_dataset['comments']):\n",
    "    secondary_dataset = secondary_dataset[secondary_dataset['comments'] != 'NaN']\n",
    "secondary_dataset = secondary_dataset[secondary_dataset['comments'].notna()]\n",
    "secondary_dataset = secondary_dataset.reset_index(drop=True)\n",
    "# secondary_dataset = secondary_dataset[['comments']]\n",
    "display(secondary_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aurel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[yg, omongan, loe, udah, jalan, waktu, ji, gab...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[jika, jalan, hidupmu, sulit, kacau, coba, rub...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[sorat, sorot, sorat, sorot, koyo, iso, mbenah...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ente, emang, paling, pinter, mengolah, kata]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[woi, dp, nol, rtemen, lu, gimana, kabare]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578</th>\n",
       "      <td>[bangga, sama, suami, yg, selalu, ingat, istri...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579</th>\n",
       "      <td>[oun, pekerjaannya, yg, penting, halal, tuk, m...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1580</th>\n",
       "      <td>[gojek, itu, mayoritas, pegangguran, yang, lam...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581</th>\n",
       "      <td>[aslinya, cantik, ayu, loh, mbak, krn, sudah, ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>[suami, saya, seumuran, sama, saya, mba, malah...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1583 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               comments  label\n",
       "0     [yg, omongan, loe, udah, jalan, waktu, ji, gab...    0.0\n",
       "1     [jika, jalan, hidupmu, sulit, kacau, coba, rub...    0.0\n",
       "2     [sorat, sorot, sorat, sorot, koyo, iso, mbenah...    0.0\n",
       "3         [ente, emang, paling, pinter, mengolah, kata]    0.0\n",
       "4            [woi, dp, nol, rtemen, lu, gimana, kabare]    0.0\n",
       "...                                                 ...    ...\n",
       "1578  [bangga, sama, suami, yg, selalu, ingat, istri...    1.0\n",
       "1579  [oun, pekerjaannya, yg, penting, halal, tuk, m...    1.0\n",
       "1580  [gojek, itu, mayoritas, pegangguran, yang, lam...    1.0\n",
       "1581  [aslinya, cantik, ayu, loh, mbak, krn, sudah, ...    1.0\n",
       "1582  [suami, saya, seumuran, sama, saya, mba, malah...    1.0\n",
       "\n",
       "[1583 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenization\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "secondary_dataset['comments'] = secondary_dataset['comments'].apply(word_tokenize)\n",
    "\n",
    "display(secondary_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5045, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slang</th>\n",
       "      <th>formal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22nya</td>\n",
       "      <td>dua-duanya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaaaaaakkk</td>\n",
       "      <td>ah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaaaahhhh</td>\n",
       "      <td>ah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aaaammmiiiinnnn</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aaahh</td>\n",
       "      <td>ah</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             slang      formal\n",
       "0            22nya  dua-duanya\n",
       "1       aaaaaaakkk          ah\n",
       "2        aaaaahhhh          ah\n",
       "3  aaaammmiiiinnnn        amin\n",
       "4            aaahh          ah"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Text Normalization / Noise Removal\n",
    "indo_slang_word = pd.read_csv(\"./Dataset/TextNormalization/lexicontest.csv\")\n",
    "\n",
    "display(indo_slang_word.shape)\n",
    "display(indo_slang_word.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def replace_slang_word(doc,slang_word):\n",
    "# #     for index in  range(0,len(doc)-1):\n",
    "# #         index_slang = slang_word.slang==doc[index]\n",
    "# #         formal = list(set(slang_word[index_slang].formal))\n",
    "# #         # replace slang word with formal word\n",
    "# #         if len(formal)>0:\n",
    "# #             doc[index] = formal[0]\n",
    "        \n",
    "# #     return doc\n",
    "# def replace_slang_word(doc, slang_word):\n",
    "#     for index in range(len(doc)):\n",
    "#         slang_match = slang_word[slang_word['slang'].isin([doc[index]])]\n",
    "#         if not slang_match.empty:\n",
    "#             formal = list(set(slang_match['formal']))\n",
    "#             if len(formal) > 0:\n",
    "#                 doc[index] = formal[0]\n",
    "#     return doc\n",
    "\n",
    "# prevent word to cut in the middle of word for example: 'kemarin' -> 'kemarin' not 'kemar' 'in'\n",
    "\n",
    "def replace_slang_word(doc, slang_word):\n",
    "    for index in range(len(doc)):\n",
    "        slang_match = slang_word[slang_word['slang'].isin([doc[index]])]\n",
    "        if not slang_match.empty:\n",
    "            formal = list(set(slang_match['formal']))\n",
    "            if len(formal) > 0:\n",
    "                doc[index] = formal[0]\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               comments  label\n",
      "0     [yg, omongan, loe, udah, jalan, waktu, ji, gab...    0.0\n",
      "1     [jika, jalan, hidupmu, sulit, kacau, coba, rub...    0.0\n",
      "2     [sorat, sorot, sorat, sorot, koyo, iso, mbenah...    0.0\n",
      "3         [ente, emang, paling, pinter, mengolah, kata]    0.0\n",
      "4            [woi, dp, nol, rtemen, lu, gimana, kabare]    0.0\n",
      "...                                                 ...    ...\n",
      "1578  [bangga, sama, suami, yg, selalu, ingat, istri...    1.0\n",
      "1579  [oun, pekerjaannya, yg, penting, halal, tuk, m...    1.0\n",
      "1580  [gojek, itu, mayoritas, pegangguran, yang, lam...    1.0\n",
      "1581  [aslinya, cantik, ayu, loh, mbak, krn, sudah, ...    1.0\n",
      "1582  [suami, saya, seumuran, sama, saya, mba, malah...    1.0\n",
      "\n",
      "[1583 rows x 2 columns]\n",
      "                                               comments  label\n",
      "0     [yang, omongan, lo, sudah, jalan, waktu, ji, g...    0.0\n",
      "1     [jika, jalan, hidupmu, sulit, kacau, coba, rub...    0.0\n",
      "2     [sorat, sorot, sorat, sorot, koyo, iso, mbenah...    0.0\n",
      "3       [anda , memang, paling, pintar, mengolah, kata]    0.0\n",
      "4      [woi, dp, nol, rtemen, kamu , bagaimana, kabare]    0.0\n",
      "...                                                 ...    ...\n",
      "1578  [bangga, sama, suami, yang, selalu, ingat, ist...    1.0\n",
      "1579  [oun, pekerjaannya, yang, penting, halal, untu...    1.0\n",
      "1580  [gojek, itu, mayoritas, pegangguran, yang, lam...    1.0\n",
      "1581  [aslinya, cantik, ayu, loh, mbak, karena, suda...    1.0\n",
      "1582  [suami, saya, seumuran, sama, saya, mbak, mala...    1.0\n",
      "\n",
      "[1583 rows x 2 columns]\n",
      "['yang', 'omongan', 'lo', 'sudah', 'jalan', 'waktu', 'ji', 'gabener', 'dki', 'bro']\n"
     ]
    }
   ],
   "source": [
    "print(secondary_dataset)\n",
    "# convert data frame to array\n",
    "# secondary_dataset['comments'] = secondary_dataset['comments'].apply(lambda x: np.array(x))\n",
    "normal_data = secondary_dataset\n",
    "normal_data = secondary_dataset['comments'].apply(replace_slang_word, slang_word=indo_slang_word)\n",
    "# append data to secondary_dataset\n",
    "print(secondary_dataset)\n",
    "\n",
    "#testing\n",
    "testing = secondary_dataset['comments'][0]\n",
    "print(testing)\n",
    "testing = replace_slang_word(testing, indo_slang_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_indo(doc):\n",
    "    return [stemmer.stem(word) for word in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yang',\n",
       " 'omongan',\n",
       " 'kamu ',\n",
       " 'sudah',\n",
       " 'jalan',\n",
       " 'waktu',\n",
       " 'ji',\n",
       " 'gabener',\n",
       " 'dki',\n",
       " 'bro']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[yang, omong, kamu, sudah, jalan, waktu, ji, g...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[jika, jalan, hidup, sulit, kacau, coba, rubah...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[sorat, sorot, sorat, sorot, koyo, iso, mbenah...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[anda, memang, paling, pintar, olah, kata]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[woi, dp, nol, rtemen, kamu, bagaimana, kabare]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578</th>\n",
       "      <td>[bangga, sama, suami, yang, selalu, ingat, ist...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579</th>\n",
       "      <td>[oun, kerja, yang, penting, halal, untuk, nafk...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1580</th>\n",
       "      <td>[gojek, itu, mayoritas, pegangguran, yang, lam...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581</th>\n",
       "      <td>[asli, cantik, ayu, loh, mbak, karena, sudah, ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>[suami, saya, umur, sama, saya, mbak, malah, t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1583 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               comments  label\n",
       "0     [yang, omong, kamu, sudah, jalan, waktu, ji, g...    0.0\n",
       "1     [jika, jalan, hidup, sulit, kacau, coba, rubah...    0.0\n",
       "2     [sorat, sorot, sorat, sorot, koyo, iso, mbenah...    0.0\n",
       "3            [anda, memang, paling, pintar, olah, kata]    0.0\n",
       "4       [woi, dp, nol, rtemen, kamu, bagaimana, kabare]    0.0\n",
       "...                                                 ...    ...\n",
       "1578  [bangga, sama, suami, yang, selalu, ingat, ist...    1.0\n",
       "1579  [oun, kerja, yang, penting, halal, untuk, nafk...    1.0\n",
       "1580  [gojek, itu, mayoritas, pegangguran, yang, lam...    1.0\n",
       "1581  [asli, cantik, ayu, loh, mbak, karena, sudah, ...    1.0\n",
       "1582  [suami, saya, umur, sama, saya, mbak, malah, t...    1.0\n",
       "\n",
       "[1583 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# stemming\n",
    "display(secondary_dataset['comments'][0])\n",
    "stemmed_data = secondary_dataset['comments'].apply(lemma_indo)\n",
    "# append stemmed data to secondary dataset, don't forget to reset index\n",
    "secondary_dataset['comments'] = stemmed_data\n",
    "secondary_dataset = secondary_dataset.reset_index(drop=True)\n",
    "display(secondary_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Minority Oversampling Technique (SMOTE)\n",
    "\n",
    "* this one need research (ivan)\n",
    "* Smote adalah sebuah tehnik yang digunakan terhadap data yang tidak seimbang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smote\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# clean empty row\n",
    "# Instantiate SMOTE\n",
    "smote = SMOTE(\n",
    "    sampling_strategy='minority',\n",
    "    random_state=42\n",
    ")\n",
    "# Assuming 'comments' is the name of the text column\n",
    "comments = secondary_dataset['comments']\n",
    "# Assuming 'label' is name of the target column\n",
    "y = secondary_dataset['label']\n",
    "\n",
    "\n",
    "# vectorizer tokenized comments\n",
    "# vectorizer = CountVectorizer()\n",
    "# TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# vectorizer = CountVectorizer()\n",
    "# X_tfidf = vectorizer.fit_transform(comments)\n",
    "joined_comments = pd.DataFrame(comments.apply(lambda x: ' '.join(x)))\n",
    "# display(joined_comments)\n",
    "# X_tfidf = vectorizer.fit_transform(joined_comments) # if we want to vectorize all comments in one row\n",
    "# fit vectorizer\n",
    "vectorizer.fit(joined_comments['comments'])\n",
    "# transform comments\n",
    "X_tfidf = vectorizer.transform(joined_comments['comments'])\n",
    "# vectorizer.fit(comments)\n",
    "# X_tfidf = vectorizer.transform(comments)\n",
    "# # display(X_tfidf)\n",
    "# # display(X_tfidf.shape)\n",
    "# Apply SMOTE to data\n",
    "X_smote, y_smote = smote.fit_resample(X_tfidf, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.69\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.64      0.69       171\n",
      "         1.0       0.64      0.74      0.69       146\n",
      "\n",
      "    accuracy                           0.69       317\n",
      "   macro avg       0.69      0.69      0.69       317\n",
      "weighted avg       0.70      0.69      0.69       317\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# without SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate Multinomial Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "# Logistic Regression\n",
    "classifier = LogisticRegression(random_state=42)\n",
    "# Fit the classifier on the training data\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.70      0.74       163\n",
      "         1.0       0.73      0.81      0.77       161\n",
      "\n",
      "    accuracy                           0.76       324\n",
      "   macro avg       0.76      0.76      0.76       324\n",
      "weighted avg       0.76      0.76      0.76       324\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate Multinomial Naive Bayes classifier\n",
    "classifier = MultinomialNB(alpha=0.1)\n",
    "\n",
    "# Fit the classifier on the training data\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters for MultinomialNB: {'alpha': 0.1, 'class_prior': None, 'fit_prior': False}\n",
      "Best accuracy for MultinomialNB: 0.7657760960134542\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# import kfold\n",
    "from sklearn.model_selection import KFold\n",
    "# Gradient Boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# All naive bayes\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Create k-Fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for combination of alpha and fit_prior\n",
    "model_param_grid = {\n",
    "    'alpha': [0.01, 0.05,0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 100.0],\n",
    "    'fit_prior': [True, False],\n",
    "    'class_prior': [None, [0.25, 0.75], [0.5, 0.5], [0.75, 0.25]]\n",
    "    }\n",
    "# model_param_grid = {\n",
    "#     'alpha': [0.01, 0.05,0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 100.0],\n",
    "#     'fit_prior': [True, False],\n",
    "#     }\n",
    "# for gaussian naive bayes\n",
    "\n",
    "model_grid_search = GridSearchCV(MultinomialNB(), model_param_grid, cv=kfold)\n",
    "model_grid_search.fit(X_smote, y_smote)\n",
    "# Print the best parameters and corresponding accuracy\n",
    "print(\"\\nBest parameters for MultinomialNB:\", model_grid_search.best_params_)\n",
    "print(\"Best accuracy for MultinomialNB:\", model_grid_search.best_score_)\n",
    "\n",
    "best_alpha = model_grid_search.best_params_['alpha']\n",
    "best_fit_prior = model_grid_search.best_estimator_.fit_prior\n",
    "\n",
    "display(best_alpha)\n",
    "display(best_fit_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation\n",
    "\n",
    "* this one need research (abhi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Accuracy = 0.7592592592592593 <== BEST RESULT\n",
      "Fold 2: Accuracy = 0.7283950617283951\n",
      "Fold 3: Accuracy = 0.8024691358024691 <== BEST RESULT\n",
      "Fold 4: Accuracy = 0.6975308641975309\n",
      "Fold 5: Accuracy = 0.7283950617283951\n",
      "Fold 6: Accuracy = 0.7716049382716049\n",
      "Fold 7: Accuracy = 0.7716049382716049\n",
      "Fold 8: Accuracy = 0.7222222222222222\n",
      "Fold 9: Accuracy = 0.8012422360248447\n",
      "Fold 10: Accuracy = 0.7763975155279503\n",
      "\n",
      "Mean Accuracy: 0.7559121233034276\n",
      "Standard Deviation: 0.03347165402877794\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "# all forest algorithm\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "# # Define 10-fold cross-validation test harness\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# # Instantiate classifier \n",
    "classifier = MultinomialNB(alpha=best_alpha, fit_prior=best_fit_prior)\n",
    "\n",
    "cv_results = cross_val_score(classifier, X_smote, y_smote, cv=kfold)\n",
    "# # Print the results for each fold\n",
    "best_fold = 0\n",
    "best_accuracy = 0\n",
    "\n",
    "for i, accuracy in enumerate(cv_results):\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_fold = i\n",
    "        print(f\"Fold {i + 1}: Accuracy = {accuracy} <== BEST RESULT\")\n",
    "    else:\n",
    "        print(f\"Fold {i + 1}: Accuracy = {accuracy}\")\n",
    "\n",
    "# from best fold, we will get the best model\n",
    "# Instantiate classifier\n",
    "# Print the mean and standard deviation of the cross-validation results\n",
    "print(f\"\\nMean Accuracy: {cv_results.mean()}\")\n",
    "print(f\"Standard Deviation: {cv_results.std()}\")\n",
    "# # After k-folds, now we will use the best model to predict the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workload Distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "<!-- * kemungkinan Binomial Naive Bayes -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1618x4394 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 21752 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets\n",
    "# secondary_dataset = secondary_dataset.reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate classifier\n",
    "classifier = MultinomialNB(alpha=best_alpha, fit_prior=best_fit_prior)\n",
    "\n",
    "# Fit the classifier on the training data\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "display(X_smote)\n",
    "\n",
    "# Contoh vektorisasi data baru\n",
    "# new_data = [\"deklarasi tukang tikung\"]\n",
    "# new_data_vectorized = vectorizer.transform(new_data)\n",
    "\n",
    "# Prediksi label data baru\n",
    "# new_data_pred = classifier.predict(new_data_vectorized)\n",
    "# print(new_data_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "classifier.fit(X_smote, y_smote)\n",
    "\n",
    "def prediction(data):    \n",
    "    tokenized = []\n",
    "    for i in range(0, len(data)):\n",
    "        # Check if the value is NaN or not a string\n",
    "        if pd.notna(data[i]) and isinstance(data[i], str):\n",
    "            tokenized.extend(word_tokenize(data[i]))\n",
    "    # Transform using the same vectorizer used during training\n",
    "    text = vectorizer.transform([\" \".join(tokenized)]).toarray()\n",
    "    # return classifier.predict(text)\n",
    "    return classifier.predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_and_save_data(X, prediction_function, output_path='./temp.csv'):\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    label = []\n",
    "    predictionLabel = []\n",
    "\n",
    "    for i in range(0, len(X)):\n",
    "        text = [X['comments'][i]]\n",
    "        pred = prediction_function(text)\n",
    "        predictionLabel.append(pred[0])\n",
    "        if pred == 1:\n",
    "            pos += 1\n",
    "        else:\n",
    "            neg += 1\n",
    "        label.append(pred[0])\n",
    "\n",
    "    X['label'] = label\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    X.to_csv(output_path, index=False)\n",
    "    print(\"Positive:\", pos)\n",
    "    print(\"Negative:\", neg)\n",
    "    # print percentage of positive and negative\n",
    "    print(\"Positive percentage:\", pos/len(X)*100)\n",
    "    print(\"Negative percentage:\", neg/len(X)*100)\n",
    "    return pos, neg, predictionLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0.0    51\n",
      "1.0    51\n",
      "Name: count, dtype: int64\n",
      "Positive: 57\n",
      "Negative: 45\n",
      "Positive percentage: 55.88235294117647\n",
      "Negative percentage: 44.11764705882353\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.75      0.79        51\n",
      "         1.0       0.77      0.86      0.81        51\n",
      "\n",
      "    accuracy                           0.80       102\n",
      "   macro avg       0.81      0.80      0.80       102\n",
      "weighted avg       0.81      0.80      0.80       102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'comments' is the text data in your dataset\n",
    "# X = pd.read_csv('./Dataset/Detik/', on_bad_lines='skip')\n",
    "X = detik_anies_after\n",
    "# clean data\n",
    "X = X.dropna()\n",
    "X = remove_punct(X)\n",
    "# norm\n",
    "X['comments'] = X['comments'].apply(word_tokenize)\n",
    "X['comments'] = X['comments'].apply(replace_slang_word, slang_word=indo_slang_word)\n",
    "X['comments'] = X['comments'].apply(lemma_indo)\n",
    "# remove stopword\n",
    "\n",
    "# convert back to string\n",
    "X['comments'] = X['comments'].apply(lambda x: ' '.join(x))\n",
    "# X['comments'] = X['comments'].apply(stopword_removal)\n",
    "print(X['label'].value_counts())\n",
    "X['comments'] = X['comments'].astype(str)\n",
    "true_label = X['label']\n",
    "pos, neg, predictionLabel = process_and_save_data(X, prediction, './temp.csv')\n",
    "\n",
    "print(\"\\n\")\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(true_label, predictionLabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    161\n",
      "1    139\n",
      "Name: count, dtype: int64\n",
      "Positive: 146\n",
      "Negative: 154\n",
      "Positive percentage: 48.66666666666667\n",
      "Negative percentage: 51.33333333333333\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.60      0.62       161\n",
      "           1       0.56      0.59      0.58       139\n",
      "\n",
      "    accuracy                           0.60       300\n",
      "   macro avg       0.60      0.60      0.60       300\n",
      "weighted avg       0.60      0.60      0.60       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'comments' is the text data in your dataset\n",
    "# X = pd.read_csv('./Dataset/Detik/', on_bad_lines='skip')\n",
    "# X = \n",
    "# clean data\n",
    "X = X.dropna()\n",
    "X = remove_punct(X)\n",
    "# norm\n",
    "X['comments'] = X['comments'].apply(word_tokenize)\n",
    "X['comments'] = X['comments'].apply(replace_slang_word, slang_word=indo_slang_word)\n",
    "X['comments'] = X['comments'].apply(lemma_indo)\n",
    "# remove stopword\n",
    "\n",
    "# convert back to string\n",
    "X['comments'] = X['comments'].apply(lambda x: ' '.join(x))\n",
    "# X['comments'] = X['comments'].apply(stopword_removal)\n",
    "print(X['label'].value_counts())\n",
    "X['comments'] = X['comments'].astype(str)\n",
    "true_label = X['label']\n",
    "pos, neg, predictionLabel = process_and_save_data(X, prediction, './temp.csv')\n",
    "\n",
    "print(\"\\n\")\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(true_label, predictionLabel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "* this one need research (pat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis the data from primary data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modal Comprarison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
