{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis dari Instagram comments calon Presiden 2024 sebelum dan sesudah deklrasi calon wakil presiden menggunakan Naive Bayes\n",
    "\n",
    "\n",
    "## Tujuan : mengetahui perbedaan sentiment dari komentar instagram sebelum dan sesudah deklarasi calon wakil presiden\n",
    "\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "    Aurelius Ivan Wijaya (00000054769)\n",
    "    </li>\n",
    "    <li>\n",
    "    Rajendra Abhinaya (00000060445)\n",
    "    </li>\n",
    "    <li>\n",
    "    Maecyntha Irelynn Tantra (00000055038)\n",
    "    </li>\n",
    "    <li>\n",
    "    Patricia theodora (00000054093)\n",
    "    </li>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "apa sih yang sebenernya kita cari?\n",
    "* web scrapping algoritm untuk data primer (ivan)\n",
    "* labeling (pat)\n",
    "* data sekunder (mae)\n",
    "* stopword library indonesia (abhi)\n",
    "* cari jurnal referensi yang sudah, sebagai literature review (all, min 4 per person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset    \n",
    "# [ Primary Dataset ]\n",
    "# anies_before = pd.read_csv('./Dataset/Anies/anies_before.csv', on_bad_lines='skip')\n",
    "# anies_after = pd.DataFrame(pd.read_csv('./Dataset/Anies/anies_after.csv', on_bad_lines=\"skip\"))\n",
    "ganjar_before = pd.DataFrame(pd.read_csv('./Dataset/Ganjar/ganjar_before.csv', on_bad_lines=\"skip\")) # get only 100 data\n",
    "ganjar_before = ganjar_before[:100]\n",
    "ganjar_after = pd.DataFrame(pd.read_csv('./Dataset/Ganjar/ganjar_after.csv', on_bad_lines=\"skip\"))\n",
    "ganjar_after = ganjar_after[:100]\n",
    "prabowo_before = pd.DataFrame(pd.read_csv('./Dataset/Prabowo/prabowo_before.csv', on_bad_lines=\"skip\"))\n",
    "prabowo_before = prabowo_before[:100]\n",
    "prabowo_after = pd.DataFrame(pd.read_csv('./Dataset/Prabowo/prabowo_after.csv', on_bad_lines=\"skip\"))\n",
    "prabowo_after = prabowo_after[:100]\n",
    "# prabowo_before = pd.DataFrame(pd.read_csv('./Dataset/Detik/detik_prabowo_before.csv', on_bad_lines=\"skip\"))\n",
    "# prabowo_after = pd.DataFrame(pd.read_csv('./Dataset/Detik/detik_prabowo_after.csv', on_bad_lines=\"skip\"   ))\n",
    "detik_anies_before = pd.read_csv('./Dataset/Detik/detik_anies_before.csv', on_bad_lines='skip')\n",
    "detik_anies_after = pd.DataFrame(pd.read_csv('./Dataset/Detik/detik_anies_before.csv', on_bad_lines=\"skip\"))\n",
    "detik_ganjar_before = pd.DataFrame(pd.read_csv('./Dataset/Detik/detik_ganjar_before.csv', on_bad_lines=\"skip\"))\n",
    "detik_ganjar_after = pd.DataFrame(pd.read_csv('./Dataset/Detik/detik_ganjar_after.csv', on_bad_lines=\"skip\"))\n",
    "detik_prabowo_before = pd.DataFrame(pd.read_csv('./Dataset/Detik/detik_prabowo_before.csv', on_bad_lines=\"skip\"))\n",
    "detik_prabowo_after = pd.DataFrame(pd.read_csv('./Dataset/Detik/detik_prabowo_after.csv', on_bad_lines=\"skip\"   ))\n",
    "# [ Secondary Dataset ]\n",
    "instagram_cyber_comments = pd.read_csv('./Dataset/dataset_komentar_instagram_cyberbullying.csv')\n",
    "tweet_tv = pd.read_csv('./Dataset/dataset_tweet_sentimen_tayangan_tv.csv')\n",
    "tweet_pilkada = pd.read_csv('./Dataset/dataset_tweet_sentiment_pilkada_DKI_2017.csv')\n",
    "tweet_opini_film = pd.read_csv('./Dataset/dataset_tweet_sentiment_opini_film.csv')\n",
    "tweet_cellular = pd.read_csv('./Dataset/dataset_tweet_sentiment_cellular_service_provider.csv')\n",
    "prastyo_sentiment = pd.read_csv('./Dataset/prastyo-sentiment_all.csv')\n",
    "sentiment_twitter_indonesia = pd.read_csv('./Dataset/Indonesian Sentiment Twitter Dataset Labeled.csv', sep='\\t')\n",
    "# sentiment_shopee = pd.read_csv('./Dataset/dataset_shopee2.csv', on_bad_lines=\"skip\")\n",
    "import chardet\n",
    "\n",
    "# Detect encoding\n",
    "with open('./Dataset/dataset_shopee2.csv', 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "# Use the detected encoding when reading the CSV file\n",
    "sentiment_shopee = pd.read_csv('./Dataset/dataset_shopee2.csv', on_bad_lines=\"skip\", encoding=result['encoding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def balance_dataset(data, label_column, max_samples=None):\n",
    "\n",
    "    # Determine the minimum number of samples for any label\n",
    "    min_samples = data[label_column].value_counts().min()\n",
    "\n",
    "    # Randomly select min_samples samples for each label\n",
    "    balanced_subset = pd.concat([group.sample(min_samples) for _, group in data.groupby(label_column)])\n",
    "\n",
    "    # Shuffle the balanced subset to mix the labels\n",
    "    balanced_subset = balanced_subset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Optionally only keep a maximum number of samples\n",
    "    if max_samples is not None:\n",
    "        balanced_subset = balanced_subset[:max_samples]\n",
    "    \n",
    "    return balanced_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: positive, 0: negative \n",
    "\n",
    "# data integration\n",
    "# change label name to 'comments'\n",
    "instagram_cyber_comments.rename(columns={'Instagram Comment Text': 'comments'}, inplace=True)\n",
    "# mapping sentiment\n",
    "instagram_cyber_comments['Sentiment'] = instagram_cyber_comments['Sentiment'].map({'positive': 1, 'negative': 0})\n",
    "instagram_cyber_comments['label'] = instagram_cyber_comments['Sentiment'].astype(int)\n",
    "# drop unused columns\n",
    "instagram_cyber_comments.drop(columns=['Id', 'Sentiment'], inplace=True)\n",
    "# display(instagram_cyber_comments.head())\n",
    "#change data type to string to ensure all data type is string\n",
    "instagram_cyber_comments['comments'] = instagram_cyber_comments['comments'].astype(str)\n",
    "\n",
    "# tweet_tv\n",
    "# change label name to 'comments'\n",
    "tweet_tv.rename(columns={'Tweet': 'comments'}, inplace=True)\n",
    "# mapping sentiment\n",
    "tweet_tv['Sentiment'] = tweet_tv['Sentiment'].map({'positive': 1, 'negative': 0})\n",
    "tweet_tv['label'] = tweet_tv['Sentiment'].astype(int)\n",
    "tweet_tv.rename(columns={'Text Tweet': 'comments'}, inplace=True)\n",
    "tweet_tv.drop(columns=['Id', 'Sentiment', \"Jumlah Retweet\", \"Acara TV\"], inplace=True)\n",
    "#change data type to string to ensure all data type is string\n",
    "tweet_tv['comments'] = tweet_tv['comments'].astype(str)\n",
    "\n",
    "# tweet_pilkada\n",
    "# tweet_pilkada add new column 'label'\n",
    "# tweet_pilkada['label'] = tweet_pilkada['Sentiment'].map({'positive': 1, 'negative': 0}).astype(int)\n",
    "tweet_pilkada['Sentiment'] = tweet_pilkada['Sentiment'].map({'positive': 1, 'negative': 0}).astype(int)\n",
    "# tweet_pilkada['label'] = tweet_pilkada['Sentiment'].astype(int)\n",
    "tweet_pilkada = tweet_pilkada[['Sentiment', 'Text Tweet']]\n",
    "tweet_pilkada.rename(columns={'Text Tweet': 'comments'}, inplace=True)\n",
    "tweet_pilkada.rename(columns={'Sentiment': 'label'}, inplace=True)\n",
    "#change data type to string to ensure all data type is string\n",
    "tweet_pilkada['comments'] = tweet_pilkada['comments'].astype(str)\n",
    "\n",
    "# tweet_opini_film\n",
    "# change label name to 'comments'\n",
    "tweet_opini_film.rename(columns={'Tweet': 'comments'}, inplace=True)\n",
    "# mapping sentiment\n",
    "tweet_opini_film['Sentiment'] = tweet_opini_film['Sentiment'].map({'positive': 1, 'negative': 0})\n",
    "tweet_opini_film['label'] = tweet_opini_film['Sentiment'].astype(int)\n",
    "tweet_opini_film.drop(columns=['Id', 'Sentiment'], inplace=True)\n",
    "tweet_opini_film.rename(columns={'Text Tweet': 'comments'}, inplace=True)\n",
    "#change data type to string to ensure all data type is string\n",
    "tweet_opini_film['comments'] = tweet_opini_film['comments'].astype(str)\n",
    "\n",
    "# tweet_cellular\n",
    "# change label name to 'comments'\n",
    "tweet_cellular.rename(columns={'Tweet': 'comments'}, inplace=True)\n",
    "# mapping sentiment\n",
    "tweet_cellular['Sentiment'] = tweet_cellular['Sentiment'].map({'positive': 1, 'negative': 0})\n",
    "tweet_cellular['label'] = tweet_cellular['Sentiment'].astype(int)\n",
    "tweet_cellular.drop(columns=['Id', 'Sentiment'], inplace=True)\n",
    "tweet_cellular.rename(columns={'Text Tweet': 'comments'}, inplace=True)\n",
    "\n",
    "# prastyo_sentiment\n",
    "map = {'pos': 1, 'neg': 0, 'neu': 1}\n",
    "prastyo_sentiment['label'] = prastyo_sentiment['label'].map(map)\n",
    "\n",
    "\n",
    "# sentiment twitter indonesia\n",
    "# change label name to 'comments'\n",
    "sentiment_twitter_indonesia.rename(columns={'Tweet': 'comments'}, inplace=True)\n",
    "# mapping sentiment\n",
    "# sentiment_twitter_indonesia['sentimen'] = sentiment_twitter_indonesia['sentimen'].map({'positive': 1, 'negative': 0})\n",
    "sentiment_twitter_indonesia['label'] = sentiment_twitter_indonesia['sentimen'].astype(int)\n",
    "\n",
    "\n",
    "# shoppe\n",
    "# change label name to 'comments'\n",
    "sentiment_shopee.rename(columns={'Review': 'comments'}, inplace=True)\n",
    "# mapping sentiment\n",
    "sentiment_shopee['SENTIMEN'] = sentiment_shopee['SENTIMEN'].map({'POSITIF': 1, 'NEGATIF': 0})\n",
    "sentiment_shopee['label'] = sentiment_shopee['SENTIMEN'].astype(int)\n",
    "sentiment_shopee.drop(columns=['SENTIMEN'], inplace=True)\n",
    "\n",
    "\n",
    "# detik_anies_after = balance_dataset(detik_anies_after, 'label')\n",
    "# detik_anies_before = balance_dataset(detik_anies_before, 'label')\n",
    "# detik_ganjar_before = balance_dataset(detik_ganjar_before, 'label', max_samples=50)\n",
    "detik_ganjar_after = balance_dataset(detik_ganjar_after, 'label', max_samples=50)\n",
    "tweet_pilkada = balance_dataset(tweet_pilkada, 'label')\n",
    "\n",
    "Training_dataset = pd.read_csv('./Dataset/Training/Training.csv', on_bad_lines='skip')\n",
    "# only pick comments and label column\n",
    "Training_dataset = Training_dataset[['comments', 'label']]\n",
    "secondary_dataset = pd.concat([\n",
    "   Training_dataset,\n",
    "    ], ignore_index=True)\n",
    "# only pick comments and label column\n",
    "secondary_dataset = secondary_dataset[['comments', 'label']]\n",
    "#change data type to string to ensure all data type is string\n",
    "secondary_dataset['comments'] = secondary_dataset['comments'].astype(str)\n",
    "# if there is column 'Unnamed: 0' in secondary dataset, drop it\n",
    "if 'Unnamed: 0' in secondary_dataset.columns:\n",
    "    secondary_dataset.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Proccessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 111 entries, 0 to 110\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   comments  111 non-null    object\n",
      " 1   label     111 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 1.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kalo saingannya pak prabowo dan pak eric ya su...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gas‚ù§ saya pilih pak ganjar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Prestasi enggak ada, suka nonton film porno pu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pak Anies Baswedan Presiden RI 2024üòç Kecerdasa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Siapapun wapresnya, Insya Allah saya akan meya...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            comments  label\n",
       "0  Kalo saingannya pak prabowo dan pak eric ya su...      0\n",
       "1                         Gas‚ù§ saya pilih pak ganjar      1\n",
       "2  Prestasi enggak ada, suka nonton film porno pu...      0\n",
       "3  Pak Anies Baswedan Presiden RI 2024üòç Kecerdasa...      1\n",
       "4  Siapapun wapresnya, Insya Allah saya akan meya...      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(secondary_dataset.info())\n",
    "display(secondary_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 111 entries, 0 to 110\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   comments  111 non-null    object\n",
      " 1   label     111 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 1.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kalo saingannya pak prabowo dan pak eric ya su...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gas‚ù§ saya pilih pak ganjar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Prestasi enggak ada, suka nonton film porno pu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pak Anies Baswedan Presiden RI 2024üòç Kecerdasa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Siapapun wapresnya, Insya Allah saya akan meya...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            comments  label\n",
       "0  Kalo saingannya pak prabowo dan pak eric ya su...      0\n",
       "1                         Gas‚ù§ saya pilih pak ganjar      1\n",
       "2  Prestasi enggak ada, suka nonton film porno pu...      0\n",
       "3  Pak Anies Baswedan Presiden RI 2024üòç Kecerdasa...      1\n",
       "4  Siapapun wapresnya, Insya Allah saya akan meya...      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def handleMissingValue(df):\n",
    "    df = df.dropna()\n",
    "    df = df.drop_duplicates()\n",
    "    df = df[df['comments'] != '']\n",
    "    # remove 'NaN' value\n",
    "    if('NaN' in df['comments']):\n",
    "        df = df[df['comments'] != 'NaN']\n",
    "    df = df[df['comments'].notna()]\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df[['comments']] \n",
    "    return df\n",
    "secondary_dataset['comments'] = handleMissingValue(secondary_dataset)\n",
    "display(secondary_dataset.info())\n",
    "display(secondary_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comments    0\n",
      "label       0\n",
      "dtype: int64\n",
      "comments    0\n",
      "label       0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kalo saingannya pak prabowo dan pak eric ya su...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gas‚ù§ saya pilih pak ganjar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Prestasi enggak ada, suka nonton film porno pu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pak Anies Baswedan Presiden RI 2024üòç Kecerdasa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Siapapun wapresnya, Insya Allah saya akan meya...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Ngebacot mah...tukang becak juga bisa ngomong ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>yah kecewa. Harusnya Pak Erick Tohir. Pasti ta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Manusia ambisius berkuasa sampai MK jadi alat ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>blunder pak jokowi, kenapa enggak erick atau m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Ogah milih lu, petugas partai, lalu pernah men...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              comments  label\n",
       "0    Kalo saingannya pak prabowo dan pak eric ya su...      0\n",
       "1                           Gas‚ù§ saya pilih pak ganjar      1\n",
       "2    Prestasi enggak ada, suka nonton film porno pu...      0\n",
       "3    Pak Anies Baswedan Presiden RI 2024üòç Kecerdasa...      1\n",
       "4    Siapapun wapresnya, Insya Allah saya akan meya...      1\n",
       "..                                                 ...    ...\n",
       "106  Ngebacot mah...tukang becak juga bisa ngomong ...      0\n",
       "107  yah kecewa. Harusnya Pak Erick Tohir. Pasti ta...      0\n",
       "108  Manusia ambisius berkuasa sampai MK jadi alat ...      0\n",
       "109  blunder pak jokowi, kenapa enggak erick atau m...      0\n",
       "110  Ogah milih lu, petugas partai, lalu pernah men...      0\n",
       "\n",
       "[111 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(secondary_dataset.isnull().sum())\n",
    "# delete duplicate data and missing value and null value\n",
    "secondary_dataset = secondary_dataset.dropna()\n",
    "secondary_dataset = secondary_dataset.drop_duplicates()\n",
    "secondary_dataset = secondary_dataset[secondary_dataset['comments'] != '']\n",
    "# remove 'NaN' value\n",
    "if('NaN' in secondary_dataset['comments']):\n",
    "    secondary_dataset = secondary_dataset[secondary_dataset['comments'] != 'NaN']\n",
    "secondary_dataset = secondary_dataset[secondary_dataset['comments'].notna()]\n",
    "secondary_dataset = secondary_dataset.reset_index(drop=True)\n",
    "# secondary_dataset = secondary_dataset[['comments']]\n",
    "print(secondary_dataset.isnull().sum())\n",
    "\n",
    "display(secondary_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Precompile regex patterns\n",
    "re_long_word = re.compile(r'\\b\\w{20,}\\b')\n",
    "re_non_ascii = re.compile(r'[^\\x00-\\x7F]+')\n",
    "# re_punctuation = re.compile(f\"[{re.escape(string.punctuation)}]\")\n",
    "re_single_char = re.compile(r'\\s+[a-zA-Z]\\s+')\n",
    "re_numbers = re.compile(r'\\d+')\n",
    "re_tags = re.compile(\"&lt;/?.*?&gt;\")\n",
    "re_special_chars_digits = re.compile(\"(\\\\d|\\\\W)+\")\n",
    "re_repeated_chars = re.compile(r'(.)\\1+')\n",
    "\n",
    "import re\n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)\n",
    "\n",
    "def clean_text(comment):\n",
    "    comment = str(comment)\n",
    "    comment = comment.lower() # Case Folding to lowercase\n",
    "\n",
    "    # Remove non-ascii words and characters\n",
    "    comment = re_non_ascii.sub(' ', comment) # remove non-ascii characters\n",
    "\n",
    "    # Remove all links\n",
    "    comment = re.sub(r\"http\\S+\", \"\", comment) # remove all links\n",
    "\n",
    "    # Delete all emoticons from text\n",
    "    comment = re.sub(':[^ ]+:', '', comment) # remove emoticons\n",
    "    comment = remove_emojis(comment)\n",
    "    # remove all word that start with < and end with >\n",
    "    comment = re.sub('<[^>]*>', '', comment)\n",
    "    # Remove long words\n",
    "    comment = re_long_word.sub('', comment) # remove long words\n",
    "\n",
    "    # if length of comment is more than 7 and ended with \"ny\" convert it to \"nya\"\n",
    "    if(len(comment) > 7 and comment[-2:] == 'ny'):\n",
    "        # comment = comment[:-2] + 'nya'\n",
    "        # add nya to the end of word\n",
    "        comment = re.sub(r'\\b(\\w+)', r'\\1nya', comment)\n",
    "    # Remove single characters\n",
    "    comment = re_single_char.sub(' ', comment) # remove single characters\n",
    "\n",
    "    # Remove numbers\n",
    "    comment = re_numbers.sub('', comment)\n",
    "\n",
    "    # Remove word that contains tags\n",
    "    comment = re_tags.sub('', comment)\n",
    "\n",
    "    # Remove special characters and digits\n",
    "    comment = re_special_chars_digits.sub(\" \", comment) \n",
    "\n",
    "    # If a letter is repeated more than 2 times, replace it with 1 time\n",
    "    comment = re_repeated_chars.sub(r'\\1\\1', comment)\n",
    "    # if comment is empty or only contain one word, return empty string\n",
    "    if(len(comment) <= 1):\n",
    "        return ''\n",
    "    # Remove extra whitespaces\n",
    "    comment = re.sub(r'\\s+', ' ', comment)\n",
    "\n",
    "    return comment.strip()\n",
    "\n",
    "def remove_punct(data):\n",
    "    data['comments'] = data['comments'].apply(clean_text)\n",
    "    # remove nan value\n",
    "    data = data[data['comments'] != '']\n",
    "    data = data[data['comments'].notna()]\n",
    "    data = data.reset_index(drop=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comments    0\n",
      "label       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Call the function\n",
    "secondary_dataset.to_csv('./dirty_data.csv', index=False)\n",
    "cleaned_data = remove_punct(secondary_dataset)\n",
    "secondary_dataset['comments'] = cleaned_data['comments']\n",
    "\n",
    "secondary_dataset = secondary_dataset[secondary_dataset['comments'] != '']\n",
    "# remove 'NaN' value\n",
    "if('NaN' in secondary_dataset['comments']):\n",
    "    secondary_dataset = secondary_dataset[secondary_dataset['comments'] != 'NaN']\n",
    "secondary_dataset = secondary_dataset[secondary_dataset['comments'].notna()]\n",
    "secondary_dataset = secondary_dataset.reset_index(drop=True)\n",
    "\n",
    "# secondary_dataset = secondary_dataset[['comments']]\n",
    "print(secondary_dataset.isnull().sum())\n",
    "# Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kalo saingannya pak prabowo dan pak eric ya su...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gas saya pilih pak ganjar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>prestasi enggak ada suka nonton film porno pul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pak anies baswedan presiden ri kecerdasan cint...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>siapapun wapresnya insya allah saya akan meyak...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>ngebacot mah tukang becak juga bisa ngomong do...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>yah kecewa harusnya pak erick tohir pasti tamb...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>manusia ambisius berkuasa sampai mk jadi alat ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>blunder pak jokowi kenapa enggak erick atau ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>ogah milih lu petugas partai lalu pernah mengg...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              comments  label\n",
       "0    kalo saingannya pak prabowo dan pak eric ya su...      0\n",
       "1                            gas saya pilih pak ganjar      1\n",
       "2    prestasi enggak ada suka nonton film porno pul...      0\n",
       "3    pak anies baswedan presiden ri kecerdasan cint...      1\n",
       "4    siapapun wapresnya insya allah saya akan meyak...      1\n",
       "..                                                 ...    ...\n",
       "106  ngebacot mah tukang becak juga bisa ngomong do...      0\n",
       "107  yah kecewa harusnya pak erick tohir pasti tamb...      0\n",
       "108  manusia ambisius berkuasa sampai mk jadi alat ...      0\n",
       "109  blunder pak jokowi kenapa enggak erick atau ma...      0\n",
       "110  ogah milih lu petugas partai lalu pernah mengg...      0\n",
       "\n",
       "[111 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(secondary_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kalo saingannya pak prabowo dan pak eric ya su...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gas saya pilih pak ganjar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>prestasi enggak ada suka nonton film porno pul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pak anies baswedan presiden ri kecerdasan cint...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>siapapun wapresnya insya allah saya akan meyak...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>ngebacot mah tukang becak juga bisa ngomong do...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>yah kecewa harusnya pak erick tohir pasti tamb...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>manusia ambisius berkuasa sampai mk jadi alat ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>blunder pak jokowi kenapa enggak erick atau ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>ogah milih lu petugas partai lalu pernah mengg...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              comments  label\n",
       "0    kalo saingannya pak prabowo dan pak eric ya su...      0\n",
       "1                            gas saya pilih pak ganjar      1\n",
       "2    prestasi enggak ada suka nonton film porno pul...      0\n",
       "3    pak anies baswedan presiden ri kecerdasan cint...      1\n",
       "4    siapapun wapresnya insya allah saya akan meyak...      1\n",
       "..                                                 ...    ...\n",
       "106  ngebacot mah tukang becak juga bisa ngomong do...      0\n",
       "107  yah kecewa harusnya pak erick tohir pasti tamb...      0\n",
       "108  manusia ambisius berkuasa sampai mk jadi alat ...      0\n",
       "109  blunder pak jokowi kenapa enggak erick atau ma...      0\n",
       "110  ogah milih lu petugas partai lalu pernah mengg...      0\n",
       "\n",
       "[111 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "display(secondary_dataset)\n",
    "# Stopword removal\n",
    "\n",
    "stopword_bahasa = pd.read_csv('./Dataset/TextNormalization/stopwordbahasa.csv', header=None)\n",
    "\n",
    "# def stopword_removal(data):\n",
    "#     for i in range(len(data)):\n",
    "#         data = data.replace(stopword_bahasa[0][i], '')\n",
    "#     return data\n",
    "\n",
    "def stopword_removal(data):\n",
    "    if isinstance(data, str):\n",
    "        # Split the string into a list of words\n",
    "        words = data.split()\n",
    "        # Remove stop words\n",
    "        words = [word for word in words if word not in stopword_bahasa[0].values]\n",
    "        # Join the list of words back into a string\n",
    "        return ' '.join(words)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kalo saingannya ya melenggang</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gas pilih</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>prestasi suka nonton film porno pulak gini cal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>presiden ri kecerdasan cinta rakyat udh no</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wapresnya insya allah orang memenangkan sbg pr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>ngebacot mah tukang becak ngomong doang mah</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>yah kecewa erick tohir kesempatan menang dibandin</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>manusia ambisius berkuasa mk alat politik kelu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>blunder erick aduh kalah aja inimah</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>ogah milih lu petugas partai menggagalkan bola...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              comments  label\n",
       "0                        kalo saingannya ya melenggang      0\n",
       "1                                            gas pilih      1\n",
       "2    prestasi suka nonton film porno pulak gini cal...      0\n",
       "3           presiden ri kecerdasan cinta rakyat udh no      1\n",
       "4    wapresnya insya allah orang memenangkan sbg pr...      1\n",
       "..                                                 ...    ...\n",
       "106        ngebacot mah tukang becak ngomong doang mah      0\n",
       "107  yah kecewa erick tohir kesempatan menang dibandin      0\n",
       "108  manusia ambisius berkuasa mk alat politik kelu...      0\n",
       "109                blunder erick aduh kalah aja inimah      0\n",
       "110  ogah milih lu petugas partai menggagalkan bola...      0\n",
       "\n",
       "[111 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kalo saingannya ya melenggang</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gas pilih</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>prestasi suka nonton film porno pulak gini cal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>presiden ri kecerdasan cinta rakyat udh no</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wapresnya insya allah orang memenangkan sbg pr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>ngebacot mah tukang becak ngomong doang mah</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>yah kecewa erick tohir kesempatan menang dibandin</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>manusia ambisius berkuasa mk alat politik kelu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>blunder erick aduh kalah aja inimah</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>ogah milih lu petugas partai menggagalkan bola...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              comments  label\n",
       "0                        kalo saingannya ya melenggang      0\n",
       "1                                            gas pilih      1\n",
       "2    prestasi suka nonton film porno pulak gini cal...      0\n",
       "3           presiden ri kecerdasan cinta rakyat udh no      1\n",
       "4    wapresnya insya allah orang memenangkan sbg pr...      1\n",
       "..                                                 ...    ...\n",
       "105        ngebacot mah tukang becak ngomong doang mah      0\n",
       "106  yah kecewa erick tohir kesempatan menang dibandin      0\n",
       "107  manusia ambisius berkuasa mk alat politik kelu...      0\n",
       "108                blunder erick aduh kalah aja inimah      0\n",
       "109  ogah milih lu petugas partai menggagalkan bola...      0\n",
       "\n",
       "[110 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Call the function\n",
    "# cleaned_data = stopword_removal(secondary_dataset)\n",
    "# convert comments to string\n",
    "# secondary_dataset['comments'] = secondary_dataset['comments'].astype(str)\n",
    "\n",
    "#check null value\n",
    "# secondary_dataset['comments'] = secondary_dataset['comments'].apply(stopword_removal)\n",
    "secondary_dataset['comments'] = secondary_dataset['comments'].apply(lambda x: stopword_removal(x))\n",
    "# secondary_dataset['comments'] = secondary_dataset['comments'].apply(lambda x: stopword_removal(x))\n",
    "display(secondary_dataset)\n",
    "\n",
    "# handle null, missing value, and duplicate data\n",
    "secondary_dataset = secondary_dataset.dropna()\n",
    "secondary_dataset = secondary_dataset.drop_duplicates()\n",
    "secondary_dataset = secondary_dataset[secondary_dataset['comments'] != '']\n",
    "# remove 'NaN' value\n",
    "if('NaN' in secondary_dataset['comments']):\n",
    "    secondary_dataset = secondary_dataset[secondary_dataset['comments'] != 'NaN']\n",
    "secondary_dataset = secondary_dataset[secondary_dataset['comments'].notna()]\n",
    "secondary_dataset = secondary_dataset.reset_index(drop=True)\n",
    "# secondary_dataset = secondary_dataset[['comments']]\n",
    "display(secondary_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aurel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[kalo, saingannya, ya, melenggang]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[gas, pilih]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[prestasi, suka, nonton, film, porno, pulak, g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[presiden, ri, kecerdasan, cinta, rakyat, udh,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[wapresnya, insya, allah, orang, memenangkan, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>[ngebacot, mah, tukang, becak, ngomong, doang,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>[yah, kecewa, erick, tohir, kesempatan, menang...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>[manusia, ambisius, berkuasa, mk, alat, politi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>[blunder, erick, aduh, kalah, aja, inimah]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>[ogah, milih, lu, petugas, partai, menggagalka...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              comments  label\n",
       "0                   [kalo, saingannya, ya, melenggang]      0\n",
       "1                                         [gas, pilih]      1\n",
       "2    [prestasi, suka, nonton, film, porno, pulak, g...      0\n",
       "3    [presiden, ri, kecerdasan, cinta, rakyat, udh,...      1\n",
       "4    [wapresnya, insya, allah, orang, memenangkan, ...      1\n",
       "..                                                 ...    ...\n",
       "105  [ngebacot, mah, tukang, becak, ngomong, doang,...      0\n",
       "106  [yah, kecewa, erick, tohir, kesempatan, menang...      0\n",
       "107  [manusia, ambisius, berkuasa, mk, alat, politi...      0\n",
       "108         [blunder, erick, aduh, kalah, aja, inimah]      0\n",
       "109  [ogah, milih, lu, petugas, partai, menggagalka...      0\n",
       "\n",
       "[110 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenization\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "secondary_dataset['comments'] = secondary_dataset['comments'].apply(word_tokenize)\n",
    "\n",
    "display(secondary_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5045, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slang</th>\n",
       "      <th>formal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22nya</td>\n",
       "      <td>dua-duanya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaaaaaakkk</td>\n",
       "      <td>ah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaaaahhhh</td>\n",
       "      <td>ah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aaaammmiiiinnnn</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aaahh</td>\n",
       "      <td>ah</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             slang      formal\n",
       "0            22nya  dua-duanya\n",
       "1       aaaaaaakkk          ah\n",
       "2        aaaaahhhh          ah\n",
       "3  aaaammmiiiinnnn        amin\n",
       "4            aaahh          ah"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Text Normalization / Noise Removal\n",
    "indo_slang_word = pd.read_csv(\"./Dataset/TextNormalization/lexicontest.csv\")\n",
    "\n",
    "display(indo_slang_word.shape)\n",
    "display(indo_slang_word.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def replace_slang_word(doc,slang_word):\n",
    "# #     for index in  range(0,len(doc)-1):\n",
    "# #         index_slang = slang_word.slang==doc[index]\n",
    "# #         formal = list(set(slang_word[index_slang].formal))\n",
    "# #         # replace slang word with formal word\n",
    "# #         if len(formal)>0:\n",
    "# #             doc[index] = formal[0]\n",
    "        \n",
    "# #     return doc\n",
    "# def replace_slang_word(doc, slang_word):\n",
    "#     for index in range(len(doc)):\n",
    "#         slang_match = slang_word[slang_word['slang'].isin([doc[index]])]\n",
    "#         if not slang_match.empty:\n",
    "#             formal = list(set(slang_match['formal']))\n",
    "#             if len(formal) > 0:\n",
    "#                 doc[index] = formal[0]\n",
    "#     return doc\n",
    "\n",
    "# prevent word to cut in the middle of word for example: 'kemarin' -> 'kemarin' not 'kemar' 'in'\n",
    "\n",
    "def replace_slang_word(doc, slang_word):\n",
    "    for index in range(len(doc)):\n",
    "        slang_match = slang_word[slang_word['slang'].isin([doc[index]])]\n",
    "        if not slang_match.empty:\n",
    "            formal = list(set(slang_match['formal']))\n",
    "            if len(formal) > 0:\n",
    "                doc[index] = formal[0]\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              comments  label\n",
      "0                   [kalo, saingannya, ya, melenggang]      0\n",
      "1                                         [gas, pilih]      1\n",
      "2    [prestasi, suka, nonton, film, porno, pulak, g...      0\n",
      "3    [presiden, ri, kecerdasan, cinta, rakyat, udh,...      1\n",
      "4    [wapresnya, insya, allah, orang, memenangkan, ...      1\n",
      "..                                                 ...    ...\n",
      "105  [ngebacot, mah, tukang, becak, ngomong, doang,...      0\n",
      "106  [yah, kecewa, erick, tohir, kesempatan, menang...      0\n",
      "107  [manusia, ambisius, berkuasa, mk, alat, politi...      0\n",
      "108         [blunder, erick, aduh, kalah, aja, inimah]      0\n",
      "109  [ogah, milih, lu, petugas, partai, menggagalka...      0\n",
      "\n",
      "[110 rows x 2 columns]\n",
      "                                              comments  label\n",
      "0                  [kalo , saingannya, ya, melenggang]      0\n",
      "1                                         [ayo, pilih]      1\n",
      "2    [prestasi, suka, nonton, film, porno, pula, be...      0\n",
      "3    [presiden, ri, kecerdasan, cinta, rakyat, suda...      1\n",
      "4    [wapresnya, insya, allah, orang, memenangkan, ...      1\n",
      "..                                                 ...    ...\n",
      "105  [ngebacot, mah, tukang, becak, mengomong, saja...      0\n",
      "106  [ya, kecewa, erick, tohir, kesempatan, menang,...      0\n",
      "107  [manusia, ambisius, berkuasa, maka, alat, poli...      0\n",
      "108        [blunder, erick, aduh, kalah, saja, inimah]      0\n",
      "109  [ogah, memilih, kamu , petugas, partai, mengga...      0\n",
      "\n",
      "[110 rows x 2 columns]\n",
      "['kalo ', 'saingannya', 'ya', 'melenggang']\n"
     ]
    }
   ],
   "source": [
    "print(secondary_dataset)\n",
    "# convert data frame to array\n",
    "# secondary_dataset['comments'] = secondary_dataset['comments'].apply(lambda x: np.array(x))\n",
    "normal_data = secondary_dataset\n",
    "normal_data = secondary_dataset['comments'].apply(replace_slang_word, slang_word=indo_slang_word)\n",
    "# append data to secondary_dataset\n",
    "print(secondary_dataset)\n",
    "\n",
    "#testing\n",
    "testing = secondary_dataset['comments'][0]\n",
    "print(testing)\n",
    "testing = replace_slang_word(testing, indo_slang_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_indo(doc):\n",
    "    return [stemmer.stem(word) for word in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kalo ', 'saingannya', 'ya', 'melenggang']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[kalo, saing, ya, lenggang]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ayo, pilih]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[prestasi, suka, nonton, film, porno, pula, be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[presiden, ri, cerdas, cinta, rakyat, sudah, no]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[wapresnya, insya, allah, orang, menang, bagai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>[ngebacot, mah, tukang, becak, omong, saja, mah]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>[ya, kecewa, erick, tohir, sempat, menang, dib...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>[manusia, ambisius, kuasa, maka, alat, politik...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>[blunder, erick, aduh, kalah, saja, inimah]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>[ogah, pilih, kamu, tugas, partai, gagal, bola...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              comments  label\n",
       "0                          [kalo, saing, ya, lenggang]      0\n",
       "1                                         [ayo, pilih]      1\n",
       "2    [prestasi, suka, nonton, film, porno, pula, be...      0\n",
       "3     [presiden, ri, cerdas, cinta, rakyat, sudah, no]      1\n",
       "4    [wapresnya, insya, allah, orang, menang, bagai...      1\n",
       "..                                                 ...    ...\n",
       "105   [ngebacot, mah, tukang, becak, omong, saja, mah]      0\n",
       "106  [ya, kecewa, erick, tohir, sempat, menang, dib...      0\n",
       "107  [manusia, ambisius, kuasa, maka, alat, politik...      0\n",
       "108        [blunder, erick, aduh, kalah, saja, inimah]      0\n",
       "109  [ogah, pilih, kamu, tugas, partai, gagal, bola...      0\n",
       "\n",
       "[110 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# stemming\n",
    "display(secondary_dataset['comments'][0])\n",
    "stemmed_data = secondary_dataset['comments'].apply(lemma_indo)\n",
    "# append stemmed data to secondary dataset, don't forget to reset index\n",
    "secondary_dataset['comments'] = stemmed_data\n",
    "secondary_dataset = secondary_dataset.reset_index(drop=True)\n",
    "display(secondary_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Minority Oversampling Technique (SMOTE)\n",
    "\n",
    "* this one need research (ivan)\n",
    "* Smote adalah sebuah tehnik yang digunakan terhadap data yang tidak seimbang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smote\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# clean empty row\n",
    "# Instantiate SMOTE\n",
    "smote = SMOTE(\n",
    "    sampling_strategy='minority',\n",
    "    random_state=42\n",
    ")\n",
    "# Assuming 'comments' is the name of the text column\n",
    "comments = secondary_dataset['comments']\n",
    "# Assuming 'label' is name of the target column\n",
    "y = secondary_dataset['label']\n",
    "\n",
    "\n",
    "# vectorizer tokenized comments\n",
    "# vectorizer = CountVectorizer()\n",
    "# TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# vectorizer = CountVectorizer()\n",
    "# X_tfidf = vectorizer.fit_transform(comments)\n",
    "joined_comments = pd.DataFrame(comments.apply(lambda x: ' '.join(x)))\n",
    "# display(joined_comments)\n",
    "# X_tfidf = vectorizer.fit_transform(joined_comments) # if we want to vectorize all comments in one row\n",
    "# fit vectorizer\n",
    "vectorizer.fit(joined_comments['comments'])\n",
    "# transform comments\n",
    "X_tfidf = vectorizer.transform(joined_comments['comments'])\n",
    "# vectorizer.fit(comments)\n",
    "# X_tfidf = vectorizer.transform(comments)\n",
    "# # display(X_tfidf)\n",
    "# # display(X_tfidf.shape)\n",
    "# Apply SMOTE to data\n",
    "X_smote, y_smote = smote.fit_resample(X_tfidf, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.59\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      1.00      0.74        13\n",
      "           1       0.00      0.00      0.00         9\n",
      "\n",
      "    accuracy                           0.59        22\n",
      "   macro avg       0.30      0.50      0.37        22\n",
      "weighted avg       0.35      0.59      0.44        22\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aurel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Aurel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Aurel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# without SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate Multinomial Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "# Logistic Regression\n",
    "classifier = LogisticRegression(random_state=42)\n",
    "# Fit the classifier on the training data\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.53      0.70        15\n",
      "           1       0.67      1.00      0.80        14\n",
      "\n",
      "    accuracy                           0.76        29\n",
      "   macro avg       0.83      0.77      0.75        29\n",
      "weighted avg       0.84      0.76      0.75        29\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate Multinomial Naive Bayes classifier\n",
    "classifier = MultinomialNB(alpha=0.1)\n",
    "\n",
    "# Fit the classifier on the training data\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters for MultinomialNB: {'alpha': 0.05, 'class_prior': [0.75, 0.25], 'fit_prior': True}\n",
      "Best accuracy for MultinomialNB: 0.9012315270935961\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# import kfold\n",
    "from sklearn.model_selection import KFold\n",
    "# Gradient Boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# All naive bayes\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Create k-Fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for combination of alpha and fit_prior\n",
    "model_param_grid = {\n",
    "    'alpha': [0.01, 0.05,0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 100.0],\n",
    "    'fit_prior': [True, False],\n",
    "    'class_prior': [None, [0.25, 0.75], [0.5, 0.5], [0.75, 0.25]]\n",
    "    }\n",
    "# model_param_grid = {\n",
    "#     'alpha': [0.01, 0.05,0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 100.0],\n",
    "#     'fit_prior': [True, False],\n",
    "#     }\n",
    "# for gaussian naive bayes\n",
    "\n",
    "model_grid_search = GridSearchCV(MultinomialNB(), model_param_grid, cv=kfold)\n",
    "model_grid_search.fit(X_smote, y_smote)\n",
    "# Print the best parameters and corresponding accuracy\n",
    "print(\"\\nBest parameters for MultinomialNB:\", model_grid_search.best_params_)\n",
    "print(\"Best accuracy for MultinomialNB:\", model_grid_search.best_score_)\n",
    "\n",
    "best_alpha = model_grid_search.best_params_['alpha']\n",
    "best_fit_prior = model_grid_search.best_estimator_.fit_prior\n",
    "\n",
    "display(best_alpha)\n",
    "display(best_fit_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation\n",
    "\n",
    "* this one need research (abhi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Accuracy = 0.6 <== BEST RESULT\n",
      "Fold 2: Accuracy = 0.8 <== BEST RESULT\n",
      "Fold 3: Accuracy = 0.7857142857142857\n",
      "Fold 4: Accuracy = 0.8571428571428571 <== BEST RESULT\n",
      "Fold 5: Accuracy = 0.7857142857142857\n",
      "Fold 6: Accuracy = 0.9285714285714286 <== BEST RESULT\n",
      "Fold 7: Accuracy = 0.7142857142857143\n",
      "Fold 8: Accuracy = 1.0 <== BEST RESULT\n",
      "Fold 9: Accuracy = 0.8571428571428571\n",
      "Fold 10: Accuracy = 0.8571428571428571\n",
      "\n",
      "Mean Accuracy: 0.8185714285714285\n",
      "Standard Deviation: 0.10518205790971409\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "# all forest algorithm\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "# # Define 10-fold cross-validation test harness\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# # Instantiate classifier \n",
    "classifier = MultinomialNB(alpha=best_alpha, fit_prior=best_fit_prior)\n",
    "\n",
    "cv_results = cross_val_score(classifier, X_smote, y_smote, cv=kfold)\n",
    "# # Print the results for each fold\n",
    "best_fold = 0\n",
    "best_accuracy = 0\n",
    "\n",
    "for i, accuracy in enumerate(cv_results):\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_fold = i\n",
    "        print(f\"Fold {i + 1}: Accuracy = {accuracy} <== BEST RESULT\")\n",
    "    else:\n",
    "        print(f\"Fold {i + 1}: Accuracy = {accuracy}\")\n",
    "\n",
    "# from best fold, we will get the best model\n",
    "# Instantiate classifier\n",
    "# Print the mean and standard deviation of the cross-validation results\n",
    "print(f\"\\nMean Accuracy: {cv_results.mean()}\")\n",
    "print(f\"Standard Deviation: {cv_results.std()}\")\n",
    "# # After k-folds, now we will use the best model to predict the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workload Distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "<!-- * kemungkinan Binomial Naive Bayes -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "classifier.fit(X_smote, y_smote)\n",
    "\n",
    "def prediction(data):    \n",
    "    tokenized = []\n",
    "    for i in range(0, len(data)):\n",
    "        # Check if the value is NaN or not a string\n",
    "        if pd.notna(data[i]) and isinstance(data[i], str):\n",
    "            tokenized.extend(word_tokenize(data[i]))\n",
    "    # Transform using the same vectorizer used during training\n",
    "    text = vectorizer.transform([\" \".join(tokenized)]).toarray()\n",
    "    # return classifier.predict(text)\n",
    "    return classifier.predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_and_save_data(X, prediction_function, output_path='./temp.csv'):\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    label = []\n",
    "    predictionLabel = []\n",
    "\n",
    "    for i in range(0, len(X)):\n",
    "        text = [X['comments'][i]]\n",
    "        pred = prediction_function(text)\n",
    "        predictionLabel.append(pred[0])\n",
    "        if pred == 1:\n",
    "            pos += 1\n",
    "        else:\n",
    "            neg += 1\n",
    "        label.append(pred[0])\n",
    "\n",
    "    X['label'] = label\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    X.to_csv(output_path, index=False)\n",
    "    print(\"Positive:\", pos)\n",
    "    print(\"Negative:\", neg)\n",
    "    # print percentage of positive and negative\n",
    "    print(\"Positive percentage:\", pos/len(X)*100)\n",
    "    print(\"Negative percentage:\", neg/len(X)*100)\n",
    "    return pos, neg, predictionLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aurel\\AppData\\Local\\Temp\\ipykernel_24520\\3654328601.py:84: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['comments'] = data['comments'].apply(clean_text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0.0    305\n",
      "1.0     48\n",
      "Name: count, dtype: int64\n",
      "Positive: 86\n",
      "Negative: 267\n",
      "Positive percentage: 24.362606232294617\n",
      "Negative percentage: 75.63739376770539\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.79      0.84       305\n",
      "         1.0       0.24      0.44      0.31        48\n",
      "\n",
      "    accuracy                           0.74       353\n",
      "   macro avg       0.57      0.61      0.58       353\n",
      "weighted avg       0.81      0.74      0.77       353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'comments' is the text data in your dataset\n",
    "# X = pd.read_csv('./Dataset/Detik/', on_bad_lines='skip')\n",
    "X = detik_anies_before\n",
    "# clean data\n",
    "X = X.dropna()\n",
    "X = remove_punct(X)\n",
    "# norm\n",
    "X['comments'] = X['comments'].apply(word_tokenize)\n",
    "X['comments'] = X['comments'].apply(replace_slang_word, slang_word=indo_slang_word)\n",
    "X['comments'] = X['comments'].apply(lemma_indo)\n",
    "# remove stopword\n",
    "\n",
    "# convert back to string\n",
    "X['comments'] = X['comments'].apply(lambda x: ' '.join(x))\n",
    "# X['comments'] = X['comments'].apply(stopword_removal)\n",
    "print(X['label'].value_counts())\n",
    "X['comments'] = X['comments'].astype(str)\n",
    "true_label = X['label']\n",
    "pos, neg, predictionLabel = process_and_save_data(X, prediction, './temp.csv')\n",
    "\n",
    "print(\"\\n\")\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(true_label, predictionLabel))\n",
    "\n",
    "# accuracy 75%\n",
    "# accuracy 76%\n",
    "# accuracy 74%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    72\n",
      "1    44\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    72\n",
      "1    44\n",
      "Name: count, dtype: int64\n",
      "Positive: 43\n",
      "Negative: 73\n",
      "Positive percentage: 37.06896551724138\n",
      "Negative percentage: 62.93103448275862\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.92      0.91        72\n",
      "           1       0.86      0.84      0.85        44\n",
      "\n",
      "    accuracy                           0.89       116\n",
      "   macro avg       0.88      0.88      0.88       116\n",
      "weighted avg       0.89      0.89      0.89       116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'comments' is the text data in your dataset\n",
    "# X = pd.read_csv('./Dataset/Detik/', on_bad_lines='skip')\n",
    "X = detik_ganjar_before\n",
    "# see how much true label\n",
    "print(X['label'].value_counts())\n",
    "# clean data\n",
    "X = X.dropna()\n",
    "X = remove_punct(X)\n",
    "# norm\n",
    "X['comments'] = X['comments'].apply(word_tokenize)\n",
    "X['comments'] = X['comments'].apply(replace_slang_word, slang_word=indo_slang_word)\n",
    "X['comments'] = X['comments'].apply(lemma_indo)\n",
    "# remove stopword\n",
    "\n",
    "# convert back to string\n",
    "X['comments'] = X['comments'].apply(lambda x: ' '.join(x))\n",
    "# X['comments'] = X['comments'].apply(stopword_removal)\n",
    "print(X['label'].value_counts())\n",
    "X['comments'] = X['comments'].astype(str)\n",
    "true_label = X['label']\n",
    "pos, neg, predictionLabel = process_and_save_data(X, prediction, './temp.csv')\n",
    "\n",
    "print(\"\\n\")\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(true_label, predictionLabel))\n",
    "\n",
    "# accuracy 87%\n",
    "# accuracy 90%\n",
    "# accuracy 89%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    72\n",
      "1    39\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    72\n",
      "1    39\n",
      "Name: count, dtype: int64\n",
      "Positive: 38\n",
      "Negative: 73\n",
      "Positive percentage: 34.234234234234236\n",
      "Negative percentage: 65.76576576576578\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97        72\n",
      "           1       0.95      0.92      0.94        39\n",
      "\n",
      "    accuracy                           0.95       111\n",
      "   macro avg       0.95      0.95      0.95       111\n",
      "weighted avg       0.95      0.95      0.95       111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = Training_dataset\n",
    "# see how much true label\n",
    "print(X['label'].value_counts())\n",
    "# clean data\n",
    "X = X.dropna()\n",
    "X = remove_punct(X)\n",
    "# norm\n",
    "X['comments'] = X['comments'].apply(word_tokenize)\n",
    "X['comments'] = X['comments'].apply(replace_slang_word, slang_word=indo_slang_word)\n",
    "X['comments'] = X['comments'].apply(lemma_indo)\n",
    "# remove stopword\n",
    "\n",
    "# convert back to string\n",
    "X['comments'] = X['comments'].apply(lambda x: ' '.join(x))\n",
    "# X['comments'] = X['comments'].apply(stopword_removal)\n",
    "print(X['label'].value_counts())\n",
    "X['comments'] = X['comments'].astype(str)\n",
    "true_label = X['label']\n",
    "pos, neg, predictionLabel = process_and_save_data(X, prediction, './temp.csv')\n",
    "\n",
    "print(\"\\n\")\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(true_label, predictionLabel))\n",
    "\n",
    "# accuracy 90%\n",
    "# accuracy 94%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0.0    319\n",
      "1.0     51\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aurel\\AppData\\Local\\Temp\\ipykernel_24520\\3654328601.py:84: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['comments'] = data['comments'].apply(clean_text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0.0    305\n",
      "1.0     48\n",
      "Name: count, dtype: int64\n",
      "Positive: 86\n",
      "Negative: 267\n",
      "Positive percentage: 24.362606232294617\n",
      "Negative percentage: 75.63739376770539\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.79      0.84       305\n",
      "         1.0       0.24      0.44      0.31        48\n",
      "\n",
      "    accuracy                           0.74       353\n",
      "   macro avg       0.57      0.61      0.58       353\n",
      "weighted avg       0.81      0.74      0.77       353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = detik_anies_after\n",
    "# see how much true label\n",
    "print(X['label'].value_counts())\n",
    "# clean data\n",
    "X = X.dropna()\n",
    "X = remove_punct(X)\n",
    "# norm\n",
    "X['comments'] = X['comments'].apply(word_tokenize)\n",
    "X['comments'] = X['comments'].apply(replace_slang_word, slang_word=indo_slang_word)\n",
    "X['comments'] = X['comments'].apply(lemma_indo)\n",
    "# remove stopword\n",
    "\n",
    "# convert back to string\n",
    "X['comments'] = X['comments'].apply(lambda x: ' '.join(x))\n",
    "# X['comments'] = X['comments'].apply(stopword_removal)\n",
    "print(X['label'].value_counts())\n",
    "X['comments'] = X['comments'].astype(str)\n",
    "true_label = X['label']\n",
    "pos, neg, predictionLabel = process_and_save_data(X, prediction, './temp.csv')\n",
    "\n",
    "print(\"\\n\")\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(true_label, predictionLabel))\n",
    "\n",
    "# accuracy 76%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "* this one need research (pat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis the data from primary data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modal Comprarison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
